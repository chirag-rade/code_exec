{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from Models.models import LLMModel\n",
    "from Agents.schemas import *\n",
    "from Agents.tools import *\n",
    "from langgraph.prebuilt.tool_executor import ToolExecutor\n",
    "from dotenv import load_dotenv\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain.output_parsers.openai_tools import PydanticToolsParser\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import (\n",
    "    AIMessage,\n",
    "    FunctionMessage,\n",
    "    HumanMessage,\n",
    ")\n",
    "from langchain.tools.render import format_tool_to_openai_function\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.pydantic_v1 import BaseModel, Field, validator\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"As a seasoned Python and JavaScript developer, you are known as code_runner. Your role involves the following steps:\"\n",
    "            \"1. You will be given a piece of code along with a test scenario, which could be a happy path or an edge case.\"\n",
    "            \"2. Your task is to write the test code and combine it with the original code to form a complete script.\"\n",
    "            \"3. If the test requires dummy files such as JSON or CSV use the write_file tool to create them and you can use create_directory tool to create a directory\"\n",
    "            \"4. Execute the complete code and test using the code_exec tool. This approach eliminates the need for any unit test framework.\"\n",
    "            \"5. If you encounter any issues after invoking any tool, feel free to make necessary corrections and retry.\"\n",
    "            \"NOTE: Do whatever you need to do and only give your final comment when done\"\n",
    "            \"Remember, always write a complete code that can be executed as a standalone script. Do not modify the original code being tested.\"\n",
    "            \"NOTE: make use of  print within your code to output information for better observation and debugging.\"\n",
    "            \"NOTE: Avoid using 'if __name__ == '__main__' as this will prevent the code from running.\"\n",
    "            \"Finally, report if the test passed and any other comment you have using result tool and nothing else\"\n",
    "            \n",
    "            \n",
    "\n",
    "            \n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\")\n",
    "    ])\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=TestResults)\n",
    "standard_test = parser.get_format_instructions()\n",
    "code_prompt = code_prompt.partial(schema = standard_test )\n",
    "llm3 = ChatOpenAI(model=\"gpt-4o\", \n",
    "                  # model_kwargs = {\"response_format\":{\"type\": \"json_object\"}}\n",
    "                  temperature=0.1\n",
    "                 )\n",
    "model3= code_prompt | llm3.bind_tools(all_schema)|PydanticToolsParser(tools=all_schema)\n",
    "model3= model3.with_retry(stop_after_attempt=3)\n",
    "\n",
    "\n",
    "\n",
    "def code_runner(state):\n",
    "    out = model3.invoke(state[\"messages\"])\n",
    "    print(out)\n",
    "    if len(out)==0:\n",
    "        return {\n",
    "        \"messages\":[],\n",
    "        \"sender\": \"code_runner\",\n",
    "    }\n",
    "    tool_name = out[-1].__class__.__name__\n",
    "    if tool_name != \"TestResults\":\n",
    "        ak = {\"function_call\":{\"arguments\":json.dumps(out[-1].dict()), \"name\": tool_name_schema_map[tool_name]}}\n",
    "        message = [AIMessage(content = \"\", additional_kwargs=ak)]\n",
    "    else:\n",
    "        content = json.dumps(out[-1].dict())\n",
    "        message = [AIMessage(content=content)]\n",
    "\n",
    "    return {\n",
    "        \"messages\":message,\n",
    "        \"sender\": \"code_runner\",\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt.tool_executor import ToolExecutor, ToolInvocation\n",
    "from Agents.agents import tool_executor\n",
    "\n",
    "\n",
    "def tool_node(state):\n",
    "    \"\"\"This runs tools in the graph\n",
    "\n",
    "    It takes in an agent action and calls that tool and returns the result.\"\"\"\n",
    "    \n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "    tool_input = last_message.additional_kwargs[\"function_call\"][\"arguments\"]\n",
    "    tool_input = json.loads(tool_input)\n",
    "    #print(tool_input), type(tool_input)\n",
    "    # We can pass single-arg inputs by value\n",
    "    # if len(tool_input) == 1 in tool_input:\n",
    "    #     tool_input = next(iter(tool_input.values()))\n",
    "    tool_name = last_message.additional_kwargs[\"function_call\"][\"name\"]\n",
    "    \n",
    "    action = ToolInvocation(\n",
    "        tool=tool_name,\n",
    "        tool_input=tool_input,\n",
    "    )\n",
    "    # We call the tool_executor and get back a response\n",
    "    response = tool_executor.invoke(action)\n",
    "    # We use the response to create a FunctionMessage\n",
    "    function_message = FunctionMessage(\n",
    "        content=f\"{tool_name} response: {str(response)}\", name=action.tool\n",
    "    )\n",
    "    # We return a list, because this will get added to the existing list\n",
    "    return {\"messages\": [function_message]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WriteFile(file_name='example.txt', content='hy man')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content=\"writ this file into txt file: 'hy man'\"),\n",
       " AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\"file_name\": \"example.txt\", \"content\": \"hy man\"}', 'name': 'write_file'}})]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [HumanMessage(\"writ this file into txt file: 'hy man'\")]\n",
    "result = code_runner({\"messages\":messages})\n",
    "messages.append(result[\"messages\"][-1])\n",
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content=\"writ this file into txt file: 'hy man'\"),\n",
       " AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\"file_name\": \"example.txt\", \"content\": \"hy man\"}', 'name': 'write_file'}}),\n",
       " FunctionMessage(content=\"write_file response: {'full_path': 'Playground/example.txt', 'status': 'File created successfully'}\", name='write_file')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_result = tool_node({\"messages\":messages})\n",
    "tool_result[\"messages\"][-1]\n",
    "messages.append(tool_result[\"messages\"][-1])\n",
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "result = code_runner({\"messages\":messages})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_dict = {tool.__name__: tool for tool in all_schema}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'CodeExec': Agents.tools.CodeExec,\n",
       " 'TestResults': Agents.tools.TestResults,\n",
       " 'WriteFile': Agents.tools.WriteFile,\n",
       " 'CreateDir': Agents.tools.CreateDir}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from tying import List\n",
      "\n",
      "def squared_sum(arr: List[int]) -> int:\n",
      "    \"\"\"\n",
      "    Plan : The problem is to find the subarray with maximum sum and return its square\n",
      "    \"\"\"\n",
      "    \"Iteration 1\"\n",
      "    # value = 0\n",
      "    # length = len(arr)\n",
      "    # for i in range(length):\n",
      "    #     if arr[i] < 1:\n",
      "    #         continue\n",
      "    #     for j in range(i+1,length+1):\n",
      "    #         value = max(value, sum(arr[i:j]))\n",
      "        \n",
      "    # return value*value\n",
      "\n",
      "    \"This approach passes 70% of test cases. I think the error lies in the complexity of the algorithm\"\n",
      "\n",
      "    \"Iteration 2\"\n",
      "    # value,temp =  0,0\n",
      "    # length = len(arr)\n",
      "    # for i in range(length):\n",
      "    #     k = temp + arr[i]\n",
      "    #     if k < 1:\n",
      "    #         temp = 0\n",
      "    #     else:\n",
      "    #         temp = k\n",
      "    #     value = max(temp, value)\n",
      "\n",
      "    \"This approach passes 83% of test cases. I will modify the logic more\"\n",
      "\n",
      "    \"Iteration 3\"\n",
      "    # initiated two varibles value and temp with 0\n",
      "    value, temp = 0, 0\n",
      "    #found the length of the array\n",
      "    length = len(arr)\n",
      "    #traverse the array\n",
      "    for i in range(length):\n",
      "        # found the current sum\n",
      "        temp += arr[i]\n",
      "        #check if current sum > 0. If yes add the value. Else reset the temp to 0\n",
      "        temp = max(temp , 0)\n",
      "        #When ever the current sum is greater than value then put there\n",
      "        value = max(temp, value)\n",
      "    #return the square\n",
      "\n",
      "    \"this approach works fine but passes only 83%.Time complexity O(N) \"\n",
      "    return value * value\n",
      "\n",
      "\n",
      "# R E A D M E\n",
      "# DO NOT CHANGE the code below, we use it to grade your submission. If changed your submission will be failed automatically.\n",
      "if __name__ == '__main__':  \n",
      "    line = input()\n",
      "    arr = [int(i) for i in line.strip().split()]\n",
      "    print(squared_sum(arr))\n"
     ]
    }
   ],
   "source": [
    "a = {\n",
    "    \"code\": \"from tying import List\\n\\ndef squared_sum(arr: List[int]) -> int:\\n    \\\"\\\"\\\"\\n    Plan : The problem is to find the subarray with maximum sum and return its square\\n    \\\"\\\"\\\"\\n    \\\"Iteration 1\\\"\\n    # value = 0\\n    # length = len(arr)\\n    # for i in range(length):\\n    #     if arr[i] < 1:\\n    #         continue\\n    #     for j in range(i+1,length+1):\\n    #         value = max(value, sum(arr[i:j]))\\n        \\n    # return value*value\\n\\n    \\\"This approach passes 70% of test cases. I think the error lies in the complexity of the algorithm\\\"\\n\\n    \\\"Iteration 2\\\"\\n    # value,temp =  0,0\\n    # length = len(arr)\\n    # for i in range(length):\\n    #     k = temp + arr[i]\\n    #     if k < 1:\\n    #         temp = 0\\n    #     else:\\n    #         temp = k\\n    #     value = max(temp, value)\\n\\n    \\\"This approach passes 83% of test cases. I will modify the logic more\\\"\\n\\n    \\\"Iteration 3\\\"\\n    # initiated two varibles value and temp with 0\\n    value, temp = 0, 0\\n    #found the length of the array\\n    length = len(arr)\\n    #traverse the array\\n    for i in range(length):\\n        # found the current sum\\n        temp += arr[i]\\n        #check if current sum > 0. If yes add the value. Else reset the temp to 0\\n        temp = max(temp , 0)\\n        #When ever the current sum is greater than value then put there\\n        value = max(temp, value)\\n    #return the square\\n\\n    \\\"this approach works fine but passes only 83%.Time complexity O(N) \\\"\\n    return value * value\\n\\n\\n# R E A D M E\\n# DO NOT CHANGE the code below, we use it to grade your submission. If changed your submission will be failed automatically.\\nif __name__ == '__main__':  \\n    line = input()\\n    arr = [int(i) for i in line.strip().split()]\\n    print(squared_sum(arr))\",\n",
    "    \"input\": \"1 -1 1 -1 1\"\n",
    "}\n",
    "print(a[\"code\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"code\": \"def test_code():\\n    print('hello world 2')\\n\\n# R E A D M E\\n# DO NOT CHANGE the code below, we use it to grade your submission. If changed your submission will be failed automatically.\\nif __name__ == '__main__':  \\n    test_code()\", \"input\": \"None\"}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def create_json(code):\n",
    "    code = \"def test_code():\\n    \" + code.replace(\"\\n\", \"\\n    \")\n",
    "    code += \"\\n\\n# R E A D M E\\n# DO NOT CHANGE the code below, we use it to grade your submission. If changed your submission will be failed automatically.\\nif __name__ == '__main__':  \\n    test_code()\"\n",
    "    return json.dumps({\"code\": code, \"input\": \"None\"})\n",
    "\n",
    "# Test the function\n",
    "code = \"print('hello world 2')\"\n",
    "print(create_json(code))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, json\n",
    "\n",
    "def create_json_for_code_api(code):\n",
    "    code = \"def test_code():\\n    \" + code.replace(\"\\n\", \"\\n    \")\n",
    "    code += \"\\n\\n# R E A D M E\\n# DO NOT CHANGE the code below, we use it to grade your submission. If changed your submission will be failed automatically.\\nif __name__ == '__main__':  \\n    test_code()\"\n",
    "    return json.dumps({\"code\": code, \"input\": \"None\"})\n",
    "\n",
    "\n",
    "\n",
    "def make_code_exec_request(code, language):\n",
    "    url = \"https://code-exec-server-staging.turing.com/api/languages/{language}\".format(language=language)\n",
    "    json_body = create_json_for_code_api(code)\n",
    "    headers = {'Content-Type': 'application/json'}\n",
    "    response = requests.post(url, data=json_body, headers=headers)\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the function\n",
    "code = '''import subprocess\\nimport sys\\n\\n# Step 1: Install django-tenant-schemas library using pip\\ndef install_django_tenant_schemas():\\n    try:\\n        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"django-tenant-schemas\"])\\n        print(\"django-tenant-schemas installed successfully.\")\\n    except subprocess.CalledProcessError as e:\\n        print(f\"Failed to install django-tenant-schemas: {e}\")\\n        sys.exit(1)\\n\\ninstall_django_tenant_schemas()'''\n",
    "a  = create_json_for_code_api(code=code)\n",
    "#make_code_exec_request(code, \"python3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"code\": \"def test_code():\\\\n    import subprocess\\\\n    import sys\\\\n    \\\\n    # Step 1: Install django-tenant-schemas library using pip\\\\n    def install_django_tenant_schemas():\\\\n        try:\\\\n            subprocess.check_call([sys.executable, \\\\\"-m\\\\\", \\\\\"pip\\\\\", \\\\\"install\\\\\", \\\\\"django-tenant-schemas\\\\\"])\\\\n            print(\\\\\"django-tenant-schemas installed successfully.\\\\\")\\\\n        except subprocess.CalledProcessError as e:\\\\n            print(f\\\\\"Failed to install django-tenant-schemas: {e}\\\\\")\\\\n            sys.exit(1)\\\\n    \\\\n    install_django_tenant_schemas()\\\\n\\\\n# R E A D M E\\\\n# DO NOT CHANGE the code below, we use it to grade your submission. If changed your submission will be failed automatically.\\\\nif __name__ == \\'__main__\\':  \\\\n    test_code()\", \"input\": \"None\"}'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "evaluator",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
