{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from Models.models import LLMModel\n",
    "from Agents.schemas import *\n",
    "from Agents.tools import *\n",
    "from langgraph.prebuilt.tool_executor import ToolExecutor\n",
    "from dotenv import load_dotenv\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain.output_parsers.openai_tools import PydanticToolsParser\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import (\n",
    "    AIMessage,\n",
    "    FunctionMessage,\n",
    "    HumanMessage,\n",
    ")\n",
    "from langchain.tools.render import format_tool_to_openai_function\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.pydantic_v1 import BaseModel, Field, validator\n",
    "\n",
    "\n",
    "class TestResult(Enum):\n",
    "    PASSED = \"Passed\"\n",
    "    FAILED = \"Failed\"\n",
    "\n",
    "class TestResults(BaseModel):\n",
    "    \"\"\"Represents the outcome of a test.\"\"\"\n",
    "    result: str = Field(\n",
    "        ..., description=\"The result of the test, either PASSED or FAILED.\"\n",
    "    )\n",
    "    comment: str = Field(\n",
    "        ..., description=\"Any comments or notes about the test result.\"\n",
    "    )\n",
    "\n",
    "class CodeExec(BaseModel):\n",
    "    __name__ = \"code_exec\"\n",
    "    \"\"\"Run Code\"\"\"\n",
    "\n",
    "    code: str = Field(description=\"The code to be executed\")\n",
    "    language: str =  Field(description=\"The language of the code.\")\n",
    "\n",
    "\n",
    "\n",
    "class WriteFile(BaseModel):\n",
    "    __name__ = \"write_file\"\n",
    "    \"\"\"Write File\"\"\"\n",
    "\n",
    "    name: str = Field(description=\"The name of the file.\")\n",
    "    content: str = Field(description=\"The content of the file.\")\n",
    "\n",
    "class CreateDir(BaseModel):\n",
    "    __name__ = \"create_directory\"\n",
    "    \"\"\"Create Directory\"\"\"\n",
    "\n",
    "    name: str = Field(description=\"The name of the directory.\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "all_schema= [CodeExec, TestResults]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"As a seasoned Python and JavaScript developer, you are known as code_runner. Your role involves the following steps:\"\n",
    "            \"1. You will be given a piece of code along with a test scenario, which could be a happy path or an edge case.\"\n",
    "            \"2. Your task is to write the test code and combine it with the original code to form a complete script.\"\n",
    "            \"3. If the test requires dummy files such as JSON or CSV use the write_file tool to create them and you can use create_directory tool to create a directory\"\n",
    "            \"4. Execute the complete code and test using the code_exec tool. This approach eliminates the need for any unit test framework.\"\n",
    "            \"5. If you encounter any issues after invoking any tool, feel free to make necessary corrections and retry.\"\n",
    "            \"NOTE: Do whatever you need to do and only give your final comment when done\"\n",
    "            \"Remember, always write a complete code that can be executed as a standalone script. Do not modify the original code being tested.\"\n",
    "            \"NOTE: make use of  print within your code to output information for better observation and debugging.\"\n",
    "            \"NOTE: Avoid using 'if __name__ == '__main__' as this will prevent the code from running.\"\n",
    "            \"Finally, report if the test passed and any other comment you have using result tool and nothing else\"\n",
    "            \n",
    "            \n",
    "\n",
    "            \n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\")\n",
    "    ])\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=TestResults)\n",
    "standard_test = parser.get_format_instructions()\n",
    "code_prompt = code_prompt.partial(schema = standard_test )\n",
    "llm3 = ChatOpenAI(model=\"gpt-4o\", \n",
    "                  # model_kwargs = {\"response_format\":{\"type\": \"json_object\"}}\n",
    "                  temperature=0.1\n",
    "                 )\n",
    "model3= code_prompt | llm3.bind_tools(all_schema)|PydanticToolsParser(tools=all_schema)\n",
    "model3= model3.with_retry(stop_after_attempt=3)\n",
    "\n",
    "\n",
    "\n",
    "def code_runner(state):\n",
    "    out = model3.invoke(state[\"messages\"])\n",
    "    print(out)\n",
    "    tool_name = out[-1].__class__.__name__\n",
    "    if tool_name != \"TestResults\":\n",
    "        ak = {\"function_call\":{\"arguments\":json.dumps(out[-1].dict()), \"name\": tool_name_schema_map[tool_name]}}\n",
    "        message = [AIMessage(content = \"\", additional_kwargs=ak)]\n",
    "    else:\n",
    "        content = json.dumps(out[-1].dict())\n",
    "        message = [AIMessage(content=content)]\n",
    "\n",
    "    return {\n",
    "        \"messages\":message,\n",
    "        \"sender\": \"code_runner\",\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt.tool_executor import ToolExecutor, ToolInvocation\n",
    "from Agents.agents import tool_executor\n",
    "\n",
    "\n",
    "def tool_node(state):\n",
    "    \"\"\"This runs tools in the graph\n",
    "\n",
    "    It takes in an agent action and calls that tool and returns the result.\"\"\"\n",
    "    \n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "    tool_input = last_message.additional_kwargs[\"function_call\"][\"arguments\"]\n",
    "    tool_input = json.loads(tool_input)\n",
    "    #print(tool_input), type(tool_input)\n",
    "    # We can pass single-arg inputs by value\n",
    "    if len(tool_input) == 1 in tool_input:\n",
    "        tool_input = next(iter(tool_input.values()))\n",
    "    tool_name = last_message.additional_kwargs[\"function_call\"][\"name\"]\n",
    "    \n",
    "    action = ToolInvocation(\n",
    "        tool=tool_name,\n",
    "        tool_input=tool_input,\n",
    "    )\n",
    "    # We call the tool_executor and get back a response\n",
    "    response = tool_executor.invoke(action)\n",
    "    # We use the response to create a FunctionMessage\n",
    "    function_message = FunctionMessage(\n",
    "        content=f\"{tool_name} response: {str(response)}\", name=action.tool\n",
    "    )\n",
    "    # We return a list, because this will get added to the existing list\n",
    "    return {\"messages\": [function_message]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CodeExec(code='print(\\'Hello, World!\\')\\n\\n# Test code\\nexpected_output = \\'Hello, World!\\'\\nprint(f\"Expected Output: {expected_output}\")', language='python')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content=\"test this code: print('Hello, World!')\"),\n",
       " AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\"code\": \"print(\\'Hello, World!\\')\\\\n\\\\n# Test code\\\\nexpected_output = \\'Hello, World!\\'\\\\nprint(f\\\\\"Expected Output: {expected_output}\\\\\")\", \"language\": \"python\"}', 'name': 'code_exec'}})]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [HumanMessage(\"test this code: print('Hello, World!')\")]\n",
    "result = code_runner({\"messages\":messages})\n",
    "messages.append(result[\"messages\"][-1])\n",
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'code': 'print(\\'Hello, World!\\')\\n\\n# Test code\\nexpected_output = \\'Hello, World!\\'\\nprint(f\"Expected Output: {expected_output}\")', 'language': 'python'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content=\"test this code: print('Hello, World!')\"),\n",
       " AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\"code\": \"print(\\'Hello, World!\\')\\\\n\\\\n# Test code\\\\nexpected_output = \\'Hello, World!\\'\\\\nprint(f\\\\\"Expected Output: {expected_output}\\\\\")\", \"language\": \"python\"}', 'name': 'code_exec'}}),\n",
       " FunctionMessage(content='code_exec response: Successfully executed:\\n```python\\nprint(\\'Hello, World!\\')\\n\\n# Test code\\nexpected_output = \\'Hello, World!\\'\\nprint(f\"Expected Output: {expected_output}\")\\n```\\nStdout: Hello, World!\\nExpected Output: Hello, World!\\n', name='code_exec')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_result = tool_node({\"messages\":messages})\n",
    "tool_result[\"messages\"][-1]\n",
    "messages.append(tool_result[\"messages\"][-1])\n",
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TestResults(result='PASSED', comment=\"The code executed successfully and printed 'Hello, World!' as expected.\")]\n"
     ]
    }
   ],
   "source": [
    "result = code_runner({\"messages\":messages})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from tying import List\n",
      "\n",
      "def squared_sum(arr: List[int]) -> int:\n",
      "    \"\"\"\n",
      "    Plan : The problem is to find the subarray with maximum sum and return its square\n",
      "    \"\"\"\n",
      "    \"Iteration 1\"\n",
      "    # value = 0\n",
      "    # length = len(arr)\n",
      "    # for i in range(length):\n",
      "    #     if arr[i] < 1:\n",
      "    #         continue\n",
      "    #     for j in range(i+1,length+1):\n",
      "    #         value = max(value, sum(arr[i:j]))\n",
      "        \n",
      "    # return value*value\n",
      "\n",
      "    \"This approach passes 70% of test cases. I think the error lies in the complexity of the algorithm\"\n",
      "\n",
      "    \"Iteration 2\"\n",
      "    # value,temp =  0,0\n",
      "    # length = len(arr)\n",
      "    # for i in range(length):\n",
      "    #     k = temp + arr[i]\n",
      "    #     if k < 1:\n",
      "    #         temp = 0\n",
      "    #     else:\n",
      "    #         temp = k\n",
      "    #     value = max(temp, value)\n",
      "\n",
      "    \"This approach passes 83% of test cases. I will modify the logic more\"\n",
      "\n",
      "    \"Iteration 3\"\n",
      "    # initiated two varibles value and temp with 0\n",
      "    value, temp = 0, 0\n",
      "    #found the length of the array\n",
      "    length = len(arr)\n",
      "    #traverse the array\n",
      "    for i in range(length):\n",
      "        # found the current sum\n",
      "        temp += arr[i]\n",
      "        #check if current sum > 0. If yes add the value. Else reset the temp to 0\n",
      "        temp = max(temp , 0)\n",
      "        #When ever the current sum is greater than value then put there\n",
      "        value = max(temp, value)\n",
      "    #return the square\n",
      "\n",
      "    \"this approach works fine but passes only 83%.Time complexity O(N) \"\n",
      "    return value * value\n",
      "\n",
      "\n",
      "# R E A D M E\n",
      "# DO NOT CHANGE the code below, we use it to grade your submission. If changed your submission will be failed automatically.\n",
      "if __name__ == '__main__':  \n",
      "    line = input()\n",
      "    arr = [int(i) for i in line.strip().split()]\n",
      "    print(squared_sum(arr))\n"
     ]
    }
   ],
   "source": [
    "code = {\n",
    "    \"code\": \"from tying import List\\n\\ndef squared_sum(arr: List[int]) -> int:\\n    \\\"\\\"\\\"\\n    Plan : The problem is to find the subarray with maximum sum and return its square\\n    \\\"\\\"\\\"\\n    \\\"Iteration 1\\\"\\n    # value = 0\\n    # length = len(arr)\\n    # for i in range(length):\\n    #     if arr[i] < 1:\\n    #         continue\\n    #     for j in range(i+1,length+1):\\n    #         value = max(value, sum(arr[i:j]))\\n        \\n    # return value*value\\n\\n    \\\"This approach passes 70% of test cases. I think the error lies in the complexity of the algorithm\\\"\\n\\n    \\\"Iteration 2\\\"\\n    # value,temp =  0,0\\n    # length = len(arr)\\n    # for i in range(length):\\n    #     k = temp + arr[i]\\n    #     if k < 1:\\n    #         temp = 0\\n    #     else:\\n    #         temp = k\\n    #     value = max(temp, value)\\n\\n    \\\"This approach passes 83% of test cases. I will modify the logic more\\\"\\n\\n    \\\"Iteration 3\\\"\\n    # initiated two varibles value and temp with 0\\n    value, temp = 0, 0\\n    #found the length of the array\\n    length = len(arr)\\n    #traverse the array\\n    for i in range(length):\\n        # found the current sum\\n        temp += arr[i]\\n        #check if current sum > 0. If yes add the value. Else reset the temp to 0\\n        temp = max(temp , 0)\\n        #When ever the current sum is greater than value then put there\\n        value = max(temp, value)\\n    #return the square\\n\\n    \\\"this approach works fine but passes only 83%.Time complexity O(N) \\\"\\n    return value * value\\n\\n\\n# R E A D M E\\n# DO NOT CHANGE the code below, we use it to grade your submission. If changed your submission will be failed automatically.\\nif __name__ == '__main__':  \\n    line = input()\\n    arr = [int(i) for i in line.strip().split()]\\n    print(squared_sum(arr))\",\n",
    "    \"input\": \"1 -1 1 -1 1\"\n",
    "}\n",
    "\n",
    "print(code[\"code\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "evaluator",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
