{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "from dotenv import  load_dotenv\n",
    "load_dotenv()\n",
    "from Models.models import LLMModel as LLMModel2\n",
    "import pytest\n",
    "from langchain.prompts import HumanMessagePromptTemplate\n",
    "from langchain_core.messages import (HumanMessage, AIMessage, FunctionMessage)\n",
    "from langchain.prompts import SystemMessagePromptTemplate, ChatPromptTemplate, MessagesPlaceholder\n",
    "from pydantic import BaseModel, Field, validator\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FunctionSchema(BaseModel):\n",
    "    function: str = Field(..., description=\"function to run if any, set to 'None' if you are not running any function\")\n",
    "    args: dict = Field(..., description=\"dictionary representing parameter and values\")\n",
    "    agent_response: str = Field(..., description=\"Your final response after receiving functions output\")\n",
    "    agent_name: str = Field(..., description=\"Your name\")\n",
    "    directed_to: str = Field(..., description=\"who the message is directed to (human or agent_cody)\")\n",
    "\n",
    "    # @validator('function')\n",
    "    # def validate_function(cls, v):\n",
    "    #     if not isinstance(v, str):\n",
    "    #         raise ValueError('function must be a string')\n",
    "    #     return v\n",
    "\n",
    "    # @validator('args')\n",
    "    # def validate_args(cls, v):\n",
    "    #     if not isinstance(v, list):\n",
    "    #         raise ValueError('args must be a list')\n",
    "    #     return v\n",
    "\n",
    "    # @validator('ai_notes')\n",
    "    # def validate_ai_notes(cls, v):\n",
    "    #     if not isinstance(v, str):\n",
    "    #         raise ValueError('ai_notes must be a string')\n",
    "    #     return v\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/Users/daniel/Desktop/Projects/TURING/Turing_Evaluator/Data/python_basics_&_scripting__write_code_in_python__22__24_03_2024_08_57_45_10.ipynb\", \"r\") as file:\n",
    "    convo = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    \"system\",\n",
    "    \"You are a helpful AI agent called agent_functions, that only communicates using JSON files and your job is to run any of the following functions. \"\n",
    "    \"1. python_repl: takes in a parameter code, runs it and returns the result \"\n",
    "    \"2. tavily_tool: takes in a topic, does a web search and returns the result \"\n",
    "    \"You are working along side other AI agents:\\n\"\n",
    "    \"agent_cody execute the functions with arguments you have provided, and responds with the result\"\n",
    "    \"finally you can give your final response\"\n",
    "])\n",
    "prompt_template = prompt_template.partial(main_prompt=prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing model: llama-v3-70b-instruct\n"
     ]
    }
   ],
   "source": [
    "model = \"llama-v3-70b-instruct\"\n",
    "provider = \"fireworks_api\"\n",
    "\n",
    "\n",
    "print(f\"Testing model: {model}\")\n",
    "evaluator = LLMModel2(\n",
    "    provider=provider,\n",
    "    model=model,\n",
    "    output_schema=FunctionSchema.model_json_schema(),\n",
    "    name=\"aspect_evaluator\",\n",
    "    prompt_template=prompt_template,\n",
    "    try_to_parse=True,\n",
    "    config={\n",
    "        \"retry\": 3,\n",
    "    \n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CODE EXECUTOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt.tool_executor import ToolExecutor, ToolInvocation\n",
    "from typing import Annotated, List, Sequence, Tuple, TypedDict, Union\n",
    "from langchain_core.tools import tool\n",
    "import contextlib\n",
    "import io\n",
    "\n",
    "@tool\n",
    "def python_repl(code: Annotated[str, \"The python code to execute.\"]):\n",
    "    \"\"\"Use this to execute python code when needed. If you want to see the output of a value,\n",
    "    you should print it out with `print(...)`. This is visible to the user and you.\n",
    "    \"\"\"\n",
    "    # Create StringIO objects to capture stdout and stderr\n",
    "    stdout = io.StringIO()\n",
    "    stderr = io.StringIO()\n",
    "\n",
    "    # Use context managers to redirect stdout and stderr to our StringIO objects\n",
    "    with contextlib.redirect_stdout(stdout), contextlib.redirect_stderr(stderr):\n",
    "        try:\n",
    "            # Use exec to execute the code\n",
    "            exec(code, locals())\n",
    "            result = stdout.getvalue()\n",
    "            error = stderr.getvalue()\n",
    "        except Exception as e:\n",
    "            # If an error occurs during execution, return the error message\n",
    "            return f\"Failed to execute. Error: {repr(e)}\"\n",
    "\n",
    "    # If no errors occurred, return the output\n",
    "    if error:\n",
    "        return f\"Successfully executed:\\n```python\\n{code}\\n```\\nStdout: {result}\\nStderr: {error}\"\n",
    "    else:\n",
    "        return f\"Successfully executed:\\n```python\\n{code}\\n```\\nStdout: {result}\"\n",
    "\n",
    "\n",
    "all_tools = [\n",
    "\n",
    "            python_repl,\n",
    "        ]\n",
    "tool_executor = ToolExecutor(all_tools)\n",
    "\n",
    "def tool_node(message):\n",
    "    tool_ = message\n",
    "   \n",
    "    action = ToolInvocation(\n",
    "        tool=tool_[\"function\"],\n",
    "        tool_input=tool_[\"args\"],\n",
    "    )\n",
    "    # We call the tool_executor and get back a response\n",
    "    response = tool_executor.invoke(action)\n",
    "    # We use the response to create a FunctionMessage\n",
    "    function_message = HumanMessage(\n",
    "        content=f\"{tool} response: {str(response)}\", name=action.tool\n",
    "    )\n",
    "    # We return a list, because this will get added to the existing list\n",
    "    return {\"messages\": [function_message]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'function': 'python_repl',\n",
       " 'args': {'code': 'for i in range(1, 11):\\n    print(i)'},\n",
       " 'agent_response': '',\n",
       " 'agent_name': 'agent_functions',\n",
       " 'directed_to': 'agent_cody'}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Perform evaluation\n",
    "user =  [\n",
    "    HumanMessage(\"{'prompt': 'execute code to print 1 to 10 in python', 'agent_name':'human'}\"),\n",
    "    # AIMessage(\"{'function': 'python_repl', 'args': {'code': 'for i in range(1, 3):\\n    print(i)\\n'}, 'agent_name': 'agent_functions', 'agent_response': ''}\"),\n",
    "    # HumanMessage(\"{'function': 'None', 'args': {}, 'agent_name': 'agent_cody', 'agent_response': '1\\n2\\n3'}\")\n",
    "    ]\n",
    "\n",
    "evaluation_result = evaluator(user)\n",
    "evaluation_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<function tool at 0x105d4d6c0> response: Successfully executed:\\n```python\\nfor i in range(1, 11):\\n    print(i)\\n```\\nStdout: 1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n10\\n'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_result = tool_node(evaluation_result)[\"messages\"]\n",
    "tool_result[0].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'function': 'None',\n",
       " 'args': {},\n",
       " 'agent_response': 'Code executed successfully. The output is: 1 2 3 4 5 6 7 8 9 10',\n",
       " 'agent_name': 'agent_functions',\n",
       " 'directed_to': 'human'}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator(tool_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "evaluator",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
