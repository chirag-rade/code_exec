{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from dotenv import  load_dotenv\n",
    "load_dotenv()\n",
    "from Models.models import LLMModel as LLMModel2\n",
    "import pytest\n",
    "from langchain.prompts import HumanMessagePromptTemplate\n",
    "from langchain_core.messages import (HumanMessage)\n",
    "from langchain.prompts import SystemMessagePromptTemplate, ChatPromptTemplate, MessagesPlaceholder\n",
    "from pydantic import BaseModel, Field, validator\n",
    "import json\n",
    "from Models.schemas import NotebookWiseFeedback\n",
    "\n",
    "\n",
    "\n",
    "#----------------------------------------------------------------\n",
    "class Joke(BaseModel):\n",
    "    \"\"\"Joke model\"\"\"\n",
    "    SETUP: str = Field(..., description=\"setup for the joke\")\n",
    "    PUNCHLINE: str = Field(..., description=\"punchline for the joke\")\n",
    "\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "            [\n",
    "                (\n",
    "                    \"human\",\n",
    "                    \"Given this topic={topic}, generate a joke.\"\n",
    "                ),\n",
    "                (\n",
    "                    \"human\",\n",
    "                    \"Also, make your output all upper case LIKE THIS.\"\n",
    "                )\n",
    "               \n",
    "            ]\n",
    "        )\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "conversation = {'status': 'OK',\n",
    " 'metadata': {'metadata': '# Metadata\\n\\n**Python Topics** - algorithms > by_topic > probability\\n\\n**Type** - query\\n\\n**Target Number of Turns (User + Assistant)** - 2\\n'},\n",
    " 'conversation': [{'cell_pos': 1,\n",
    "   'role': 'User',\n",
    "   'content': 'Could you show me a Python function to calculate the probability of rolling a sum of 8 with two six-sided dice?',\n",
    "   'type': 'markdown'},\n",
    "  {'cell_pos': 2,\n",
    "   'role': 'Assistant',\n",
    "   'content': 'Certainly! When you roll two six-sided dice, there are 36 possible outcomes (as there are 6 possible outcomes for each die). To determine the probability of obtaining a sum of 8, you must identify the favorable outcomes (those resulting in a sum of 8) and then divide this count by the total number of possible outcomes.\\n\\nHere is a python function for it:',\n",
    "   'type': 'markdown'},\n",
    "  {'cell_pos': 3,\n",
    "   'role': 'Assistant',\n",
    "   'content': 'def probability_of_rolling_sum_eight():\\n    favorable_outcomes = 0\\n    total_outcomes = 36 # There are 6 * 6 = 36 total outcomes\\n\\n    for dice_1 in range(1, 7):\\n        dice_2 = 8 - dice_1\\n\\n        # Check if the value is valid\\n        if 1 <= dice_2 <= 6:\\n            favorable_outcomes += 1\\n\\n    return favorable_outcomes / total_outcomes\\n\\n# Call the function and print the result\\nprobability_of_rolling_sum_eight = probability_of_rolling_sum_eight()\\n\\nprint(f\"The probability of rolling a sum of 8 with two six-sided dice is: {probability_of_rolling_sum_eight}\")',\n",
    "   'type': 'code'},\n",
    "  {'cell_pos': 4,\n",
    "   'role': 'Assistant',\n",
    "   'content': 'The time complexity of the `probability_of_rolling_sum_eight` function is O(N) since it undergoes a constant number of iterations (6 iterations, corresponding to each possible value of dice_1 ranging from 1 to 6).The loop entails basic arithmetic operations and conditional checks, all with constant time complexity.\\n\\nAs a result, the time complexity remains constant and is not influenced by the input size, irrespective of the number of sides on the dice. The function is specifically tailored for six-sided dice, ensuring a consistent and limited number of iterations.',\n",
    "   'type': 'markdown'},\n",
    "  {'cell_pos': 5,\n",
    "   'role': 'User',\n",
    "   'content': 'Could you kindly provide the result with precision up to four decimal places only?',\n",
    "   'type': 'markdown'},\n",
    "  {'cell_pos': 6,\n",
    "   'role': 'Assistant',\n",
    "   'content': \"Certainly! Here's the revised version of the function that will provide the outcome with accuracy up to four decimal places.\",\n",
    "   'type': 'markdown'},\n",
    "  {'cell_pos': 7,\n",
    "   'role': 'Assistant',\n",
    "   'content': 'def probability_of_rolling_sum_eight():\\n    favorable_outcomes = 0\\n    total_outcomes = 36 # There are 6 * 6 = 36 total outcomes\\n\\n    for die1 in range(1, 7):\\n        die2 = 8 - die1\\n\\n        # Check if the value is valid\\n        if 1 <= die2 <= 6:\\n            favorable_outcomes += 1\\n\\n    return favorable_outcomes / total_outcomes\\n\\n# Call the function and print the result\\nprobability_of_rolling_sum_eight = probability_of_rolling_sum_eight()\\n\\nprint(f\"The probability of rolling a sum of 8 with two six-sided dice is: {probability_of_rolling_sum_eight: .4f}\")',\n",
    "   'type': 'code'}]}\n",
    "\n",
    "chat_template = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            SystemMessagePromptTemplate.from_template(\n",
    "                \"\"\"Name: AI Assistant Perfector.\n",
    "Profile: You are an expert in perfecting AI Assistants' response content based on the user's expertise level.\"\"\"\n",
    "            ),\n",
    "            HumanMessagePromptTemplate.from_template(\n",
    "                \"\"\"\n",
    "Given the following conversation between Human User and AI Assistant, find issues following the rules described below and rate the total conversation.\n",
    "Single significant instance of deviation from the rules - score 1 or 2. More issues score<2. No issues=5.\n",
    "\n",
    "Qualities we care about. Focus on them and only find issues that are directly related to them:\n",
    "```\n",
    "You must assume the user just started to learn about the question that is asked, so the \n",
    "reply should cover all the points that the user might be new to, and assume the \n",
    "user has basic knowledge about the prerequisites. \n",
    "\n",
    "This helps us keep the explanation clean, and makes it useful to the user rather \n",
    "than throwing all information about the topic to the user.\n",
    "\n",
    "It is important to identify the query intent to gauge the user knowledge level as well as \n",
    "the code complexity to provide the most useful explanation.\n",
    "```\n",
    "\n",
    "The task:\n",
    "```\n",
    "Please, detect all mismatches between user's expertise level shown and the replies of the Assistant.\n",
    "If User expertise level is unknown - asumme they are a beginner in that question.\n",
    "Mismatches might include but not limited to:\n",
    "    - too much explanation for an expert\n",
    "    - too little explanation for a beginner\n",
    "    - Assistant assumes the user is not the beginner in the question asked be it an algo or a technology or something else.\n",
    "\n",
    "Assume basic knowledge of Python programming by the user and so no need to explain basic things unless asked to.\n",
    "For example, if the question is about an algorithm in python, assume understnding of Python but a beginner level in algorithms UNLESS USER SHOWS OR STATES A HIGHER OR LOWER LEVEL OF EXPERTISE.\n",
    "\n",
    "If no issues found, do not create any.\n",
    "Correctness or accuracy is not your concern and will be handled by other evaluators. Focus only on the serving user's level of expertise in the most helpful manner.\n",
    "```\n",
    "\n",
    "Conversation:\n",
    "CONVERSATION_START\n",
    "{conversation}\n",
    "CONVERSATION_END\n",
    "\n",
    "Now, proceed to completing your task of finding issues and scoring the conversation.\n",
    "\"\"\"\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "#----------------------------------------------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#JOKE SCHEMA\n",
    "\n",
    "def test_anthropic_models():\n",
    "        models = [\n",
    "            \"claude-3-opus-20240229\",\n",
    "        ]\n",
    "        provider = \"anthropic_api\"\n",
    "        print(f\"Testing models from provider: {provider}\")\n",
    "        for model in models:\n",
    "            print(f\"Testing model: {model}\")\n",
    "            evaluator = LLMModel2(\n",
    "                provider=provider,\n",
    "                model=model,\n",
    "                output_schema=Joke.model_json_schema(),\n",
    "                use_tool=False,\n",
    "                name=\"aspect_evaluator\",\n",
    "                   prompt_template=prompt_template,\n",
    "                try_to_parse=True,\n",
    "                config={\n",
    "                    \"retry\": 3,\n",
    "                    \n",
    "                }\n",
    "            )\n",
    "\n",
    "            # Perform evaluation\n",
    "            evaluation_result = evaluator({\n",
    "                \"topic\": \"dad joke\"\n",
    "            })\n",
    "            print(evaluation_result)\n",
    "            Joke.model_validate(evaluation_result)\n",
    "\n",
    "\n",
    "def test_fireworks_models():\n",
    "        models = [\n",
    "        \"llama-v3-70b-instruct\",\n",
    "    \n",
    "          #  \"dbrx-instruct\",\n",
    "            \"mixtral-8x22b-instruct\",\n",
    "            \"llama-v3-8b-instruct\"\n",
    "        ]\n",
    "        provider = \"fireworks_api\"\n",
    "        print(f\"Testing models from provider: {provider}\")\n",
    "        for model in models:\n",
    "            print(f\"Testing model: {model}\")\n",
    "            evaluator = LLMModel2(\n",
    "                provider=provider,\n",
    "                model=model,\n",
    "                output_schema=Joke.model_json_schema(),\n",
    "                name=\"aspect_evaluator\",\n",
    "                prompt_template=prompt_template,\n",
    "                try_to_parse=True,\n",
    "                config={\n",
    "                    \"retry\": 3,\n",
    "                \n",
    "                }\n",
    "            )\n",
    "\n",
    "            # Perform evaluation\n",
    "            evaluation_result = evaluator({\n",
    "                \"topic\": \"dad joke\"\n",
    "            })\n",
    "            print(evaluation_result)\n",
    "            Joke.model_validate(evaluation_result)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def test_openai_models(): \n",
    "        models = [\n",
    "             \"gpt-3.5-turbo-1106\",\n",
    "            \"gpt-4-turbo\",\n",
    "            \"gpt-3.5-turbo\",\n",
    "            \"gpt-4o\",\n",
    "        ]\n",
    "        provider = \"openai_api\"\n",
    "        print(f\"Testing models from provider: {provider}\")\n",
    "        for model in models:\n",
    "            print(f\"Testing model: {model}\")\n",
    "            evaluator = LLMModel2(\n",
    "                provider=provider,\n",
    "                model=model,\n",
    "                #use_tool=False,\n",
    "                output_schema=Joke.model_json_schema(),\n",
    "                name=\"aspect_evaluator\",\n",
    "                  prompt_template=prompt_template,\n",
    "                try_to_parse=True,\n",
    "                config={\n",
    "                    \"retry\": 4,\n",
    "                }\n",
    "            )\n",
    "\n",
    "            # Perform evaluation\n",
    "            evaluation_result = evaluator({\n",
    "               \"topic\":\"dad joke\"\n",
    "            })\n",
    "            print(evaluation_result)\n",
    "            Joke.model_validate(evaluation_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#WITH PYDANTIC(test retries/validatiion)\n",
    "\n",
    "\n",
    "\n",
    "def test_anthropic_models():\n",
    "        models = [\n",
    "            \"claude-3-opus-20240229\",\n",
    "        ]\n",
    "        provider = \"anthropic_api\"\n",
    "        print(f\"Testing models from provider: {provider}\")\n",
    "        for model in models:\n",
    "            print(f\"Testing model: {model}\")\n",
    "            evaluator = LLMModel2(\n",
    "                provider=provider,\n",
    "                model=model,\n",
    "                output_schema=NotebookWiseFeedback,\n",
    "                name=\"aspect_evaluator\",\n",
    "                prompt_template=chat_template,\n",
    "                try_to_parse=True,\n",
    "                config={\n",
    "                    \"retry\": 3,\n",
    "                    \n",
    "                }\n",
    "            )\n",
    "\n",
    "            # Perform evaluation\n",
    "            evaluation_result = evaluator({\n",
    "                \"conversation\": conversation\n",
    "            })\n",
    "            print(evaluation_result)\n",
    "            NotebookWiseFeedback.model_validate(evaluation_result)\n",
    "\n",
    "\n",
    "def test_fireworks_models():\n",
    "        models = [\n",
    "        \"llama-v3-70b-instruct\",\n",
    "            \"dbrx-instruct\",\n",
    "            \"mixtral-8x22b-instruct\",\n",
    "            \"llama-v3-8b-instruct\"\n",
    "        ]\n",
    "        provider = \"fireworks_api\"\n",
    "        print(f\"Testing models from provider: {provider}\")\n",
    "        for model in models:\n",
    "            print(f\"Testing model: {model}\")\n",
    "            evaluator = LLMModel2(\n",
    "                provider=provider,\n",
    "                model=model,\n",
    "                output_schema=NotebookWiseFeedback,\n",
    "                name=\"aspect_evaluator\",\n",
    "                prompt_template=chat_template,\n",
    "                try_to_parse=True,\n",
    "                config={\n",
    "                    \"retry\": 3,\n",
    "                \n",
    "                }\n",
    "            )\n",
    "\n",
    "            # Perform evaluation\n",
    "            evaluation_result = evaluator({\n",
    "                \"conversation\": conversation\n",
    "            })\n",
    "            print(evaluation_result)\n",
    "            NotebookWiseFeedback.model_validate(evaluation_result)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def test_openai_models(): \n",
    "        models = [\n",
    "             \"gpt-3.5-turbo-1106\",\n",
    "            \"gpt-4-turbo\",\n",
    "            \"gpt-3.5-turbo\",\n",
    "            \"gpt-4o\",\n",
    "        ]\n",
    "        provider = \"openai_api\"\n",
    "        print(f\"Testing models from provider: {provider}\")\n",
    "        for model in models:\n",
    "            print(f\"Testing model: {model}\")\n",
    "            evaluator = LLMModel2(\n",
    "                provider=provider,\n",
    "                model=model,\n",
    "                output_schema=NotebookWiseFeedback,\n",
    "                name=\"aspect_evaluator\",\n",
    "                prompt_template=chat_template,\n",
    "                try_to_parse=True,\n",
    "                config={\n",
    "                    \"retry\": 4,\n",
    "                }\n",
    "            )\n",
    "\n",
    "            # Perform evaluation\n",
    "            evaluation_result = evaluator({\n",
    "                \"conversation\": conversation\n",
    "            })\n",
    "            print(evaluation_result)\n",
    "            NotebookWiseFeedback.model_validate(evaluation_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#NOTEBOOKWISE\n",
    "\n",
    "\n",
    "\n",
    "def test_anthropic_models():\n",
    "        models = [\n",
    "            \"claude-3-opus-20240229\",\n",
    "        ]\n",
    "        provider = \"anthropic_api\"\n",
    "        print(f\"Testing models from provider: {provider}\")\n",
    "        for model in models:\n",
    "            print(f\"Testing model: {model}\")\n",
    "            evaluator = LLMModel2(\n",
    "                provider=provider,\n",
    "                model=model,\n",
    "                output_schema=NotebookWiseFeedback.model_json_schema(),\n",
    "                name=\"aspect_evaluator\",\n",
    "                prompt_template=chat_template,\n",
    "                try_to_parse=True,\n",
    "                config={\n",
    "                    \"retry\": 3,\n",
    "                    \n",
    "                }\n",
    "            )\n",
    "\n",
    "            # Perform evaluation\n",
    "            evaluation_result = evaluator({\n",
    "                \"conversation\": conversation\n",
    "            })\n",
    "            print(evaluation_result)\n",
    "            NotebookWiseFeedback.model_validate(evaluation_result)\n",
    "\n",
    "\n",
    "def test_fireworks_models():\n",
    "        models = [\n",
    "        \"llama-v3-70b-instruct\",\n",
    "            \"dbrx-instruct\",\n",
    "            \"mixtral-8x22b-instruct\",\n",
    "            \"llama-v3-8b-instruct\"\n",
    "        ]\n",
    "        provider = \"fireworks_api\"\n",
    "        print(f\"Testing models from provider: {provider}\")\n",
    "        for model in models:\n",
    "            print(f\"Testing model: {model}\")\n",
    "            evaluator = LLMModel2(\n",
    "                provider=provider,\n",
    "                model=model,\n",
    "                output_schema=NotebookWiseFeedback.model_json_schema(),\n",
    "                name=\"aspect_evaluator\",\n",
    "                prompt_template=chat_template,\n",
    "                try_to_parse=True,\n",
    "                config={\n",
    "                    \"retry\": 3,\n",
    "                \n",
    "                }\n",
    "            )\n",
    "\n",
    "            # Perform evaluation\n",
    "            evaluation_result = evaluator({\n",
    "                \"conversation\": conversation\n",
    "            })\n",
    "            print(evaluation_result)\n",
    "            NotebookWiseFeedback.model_validate(evaluation_result)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def test_openai_models(): \n",
    "        models = [\n",
    "             \"gpt-3.5-turbo-1106\",\n",
    "            \"gpt-4-turbo\",\n",
    "            \"gpt-3.5-turbo\",\n",
    "            \"gpt-4o\",\n",
    "        ]\n",
    "        provider = \"openai_api\"\n",
    "        print(f\"Testing models from provider: {provider}\")\n",
    "        for model in models:\n",
    "            print(f\"Testing model: {model}\")\n",
    "            evaluator = LLMModel2(\n",
    "                provider=provider,\n",
    "                model=model,\n",
    "                output_schema=NotebookWiseFeedback.model_json_schema(),\n",
    "                name=\"aspect_evaluator\",\n",
    "                prompt_template=chat_template,\n",
    "                try_to_parse=True,\n",
    "                config={\n",
    "                    \"retry\": 4,\n",
    "                }\n",
    "            )\n",
    "\n",
    "            # Perform evaluation\n",
    "            evaluation_result = evaluator({\n",
    "                \"conversation\": conversation\n",
    "            })\n",
    "            print(evaluation_result)\n",
    "            NotebookWiseFeedback.model_validate(evaluation_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing models from provider: openai_api\n",
      "Testing model: gpt-3.5-turbo-1106\n",
      "{'user_intents': [{'cell_pos': 1, 'intent': 'Python function to calculate probability of rolling a sum of 8 with two six-sided dice'}], 'user_expertise_level': {'inquiry_topic': 'algorithms', 'level': 'beginner'}, 'scratchpad': \"The user is inquiring about the Python function to calculate the probability of rolling a sum of 8 with two six-sided dice. The user's expertise level in algorithms is at a beginner level. The assistant should provide an explanation suitable for a beginner in algorithms and assume basic knowledge of Python programming by the user.\", 'issues': [], 'score': 5}\n",
      "Testing model: gpt-4-turbo\n",
      "{'user_intents': [{'cell_pos': 1, 'intent': 'Inquire about creating a Python function for calculating probability of rolling a sum of 8 with two dice.'}, {'cell_pos': 5, 'intent': 'Request for result precision in the provided Python function.'}], 'user_expertise_level': {'inquiry_topic': 'Python programming for probability calculations', 'level': 'beginner'}, 'scratchpad': \"The user's questions suggest they are a beginner seeking help with specific Python functions for probability calculations. The assistant's responses should cater to this beginner level, focusing on clarity and not assuming prior knowledge beyond basic Python.\", 'issues': [], 'score': 5}\n",
      "Testing model: gpt-3.5-turbo\n",
      "{'user_intents': [{'cell_pos': 1, 'intent': 'Requesting Python function to calculate probability of rolling a sum of 8 with two six-sided dice'}, {'cell_pos': 5, 'intent': 'Requesting result with precision up to four decimal places'}], 'user_expertise_level': {'inquiry_topic': 'probability of rolling a sum of 8 with two six-sided dice', 'level': 'beginner'}, 'scratchpad': 'The user is asking for a Python function to calculate the probability of rolling a sum of 8 with two six-sided dice and later requests the result with precision up to four decimal places.', 'issues': [{'type': 'Explanation Mismatch', 'reason': 'Assistant provided detailed explanation including time complexity which may be too advanced for a beginner level user', 'cell_positions': [2, 4], 'fix': 'Simplify the explanation by focusing on the basic concept of calculating the probability of rolling a sum of 8 with two six-sided dice. Avoid advanced concepts like time complexity for a beginner audience.'}], 'score': 2}\n",
      "Testing model: gpt-4o\n",
      "{'user_intents': [{'cell_pos': 1, 'intent': 'Request a Python function to calculate the probability of rolling a sum of 8 with two six-sided dice.'}, {'cell_pos': 5, 'intent': 'Request to provide the result with precision up to four decimal places.'}], 'user_expertise_level': {'inquiry_topic': 'probability of rolling a sum with dice', 'level': 'beginner'}, 'scratchpad': 'Human User has shown interest in calculating the probability of rolling a sum of 8 with two six-sided dice. The user has not shown any advanced knowledge in the topic, so they should be assumed to be a beginner.\\n\\nThe Assistant provided a detailed explanation and a Python function for the calculation. However, the explanation on time complexity seems a bit too advanced for a beginner who is just asking for a probability calculation function. This might overwhelm the user.\\n\\nAdditionally, the initial explanation of the probability calculation process and the Python function seems appropriate for a beginner.', 'issues': [{'type': 'too advanced explanation', 'reason': 'The explanation of time complexity is too advanced for a beginner who is just asking for a probability calculation function.', 'cell_positions': [4], 'fix': 'Omit the time complexity explanation or simplify it significantly, focusing only on the probability calculation.'}], 'score': 4}\n"
     ]
    }
   ],
   "source": [
    "test_openai_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing models from provider: anthropic_api\n",
      "Testing model: claude-3-opus-20240229\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/daniel/anaconda3/envs/evaluator/lib/python3.12/site-packages/langchain_core/_api/beta_decorator.py:87: LangChainBetaWarning: The method `ChatAnthropic.bind_tools` is in beta. It is actively being worked on, so the API may change.\n",
      "  warn_beta(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'user_intents': [{'cell_pos': 1, 'intent': 'Requests a Python function to calculate the probability of rolling a sum of 8 with two six-sided dice'}, {'cell_pos': 5, 'intent': 'Asks to provide the probability result with precision up to four decimal places'}], 'user_expertise_level': {'inquiry_topic': 'calculating dice roll probabilities in Python', 'level': 'beginner in the specific problem, with basic Python knowledge'}, 'scratchpad': \"The user appears to be a beginner at calculating dice roll probabilities, while having some basic Python knowledge. The assistant should provide explanations suitable for a probability beginner, using Python examples that assume foundational programming skills.\\nThe responses are generally appropriate, but the explanation of time complexity in Response 3 seems unnecessarily advanced for the user's level and request.\", 'issues': [{'type': 'Explanation too complex for beginner', 'reason': 'The discussion of time complexity is overly detailed and advanced for a beginner-level probability question.', 'cell_positions': [4], 'fix': \"Remove the time complexity analysis, as it is not directly relevant to the user's request and goes beyond a beginner level. Focus on just providing the requested probability calculation.\"}], 'score': 4}\n"
     ]
    }
   ],
   "source": [
    "test_anthropic_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing models from provider: fireworks_api\n",
      "Testing model: llama-v3-70b-instruct\n",
      "{'user_intents': [{'cell_pos': 1, 'intent': 'Calculate the probability of rolling a sum of 8 with two six-sided dice using Python'}, {'cell_pos': 5, 'intent': 'Request the result with precision up to four decimal places'}], 'user_expertise_level': {'inquiry_topic': 'Python algorithms', 'level': 'Beginner'}, 'scratchpad': \"The user is a beginner in Python algorithms. The conversation starts with a clear question about calculating the probability of rolling a sum of 8 with two six-sided dice using Python. The Assistant's responses are generally clear and concise.\", 'issues': [{'type': 'Too much explanation for a beginner', 'reason': \"The Assistant's response in cell 4 explains the time complexity of the function, which may be excessive for a beginner in Python algorithms.\", 'cell_positions': [4], 'fix': \"Remove or simplify the explanation of time complexity to better match the user's expertise level.\"}], 'score': 4}\n",
      "Testing model: dbrx-instruct\n",
      "{'user_intents': [{'cell_pos': 1, 'intent': 'request for information'}, {'cell_pos': 5, 'intent': 'request for modification'}], 'user_expertise_level': {'inquiry_topic': 'probability', 'level': 'beginner'}, 'scratchpad': \"The user is a beginner in the topic of probability. The assistant provides a clear and concise explanation of the problem and the solution. The assistant also explains the time complexity of the function, which is helpful for beginners to understand. However, the assistant assumes that the user knows what time complexity is, which might not be the case for a beginner. The assistant could have explained what time complexity is and why it is important. The assistant also provides the exact code for the solution, which is helpful for beginners. Overall, the assistant does a good job of tailoring the response to the user's expertise level.\", 'issues': [{'type': 'assumption', 'reason': 'The assistant assumes that the user knows what time complexity is, which might not be the case for a beginner.', 'cell_positions': [4], 'fix': 'The assistant could have explained what time complexity is and why it is important.'}, {'type': 'assumption', 'reason': \"The assistant assumes that the user knows what the ':.4f' format specifier in the print statement does, which might not be the case for a beginner.\", 'cell_positions': [7], 'fix': \"The assistant could have explained what the ':.4f' format specifier does.\"}], 'score': 4}\n",
      "Testing model: mixtral-8x22b-instruct\n",
      "{'user_intents': [{'cell_pos': 1, 'intent': 'request_code_snippet'}, {'cell_pos': 5, 'intent': 'request_code_modification'}], 'user_expertise_level': {'inquiry_topic': 'probability_calculation', 'level': 'beginner'}, 'scratchpad': 'The user is a beginner in probability calculation and has asked for a Python function to calculate the probability of rolling a sum of 8 with two six-sided dice. The assistant provided a code snippet and explained its time complexity. The user then asked for the result with precision up to four decimal places, and the assistant provided the modified code.', 'issues': [], 'score': 5}\n",
      "Testing model: llama-v3-8b-instruct\n",
      "{'user_intents': [], 'user_expertise_level': {'inquiry_topic': 'probability', 'level': 'beginner'}, 'scratchpad': 'No issues found', 'issues': [], 'score': 5}\n"
     ]
    }
   ],
   "source": [
    "test_fireworks_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing models from provider: fireworks_api\n",
      "Testing model: dbrx-instruct\n",
      "Validating output schema.....\n",
      "{'user_intents': [{'cell_pos': 1, 'intent': 'Request for a Python function to calculate the probability of rolling a sum of 8 with two six-sided dice'}, {'cell_pos': 5, 'intent': 'Request for a more precise result (up to four decimal places)'}], 'user_expertise_level': {'inquiry_topic': 'Probability of rolling a sum with two six-sided dice', 'level': 'Beginner'}, 'scratchpad': 'The user is a beginner in the topic of probability and has requested a Python function to calculate the probability of rolling a sum of 8 with two six-sided dice. The user also requested a more precise result.', 'issues': [], 'score': 5}\n"
     ]
    }
   ],
   "source": [
    "models = [\n",
    "        #\"llama-v3-70b-instruct\",\n",
    "            \"dbrx-instruct\",\n",
    "         #   \"mixtral-8x22b-instruct\",\n",
    "         #   \"llama-v3-8b-instruct\"\n",
    "        ]\n",
    "provider = \"fireworks_api\"\n",
    "print(f\"Testing models from provider: {provider}\")\n",
    "for model in models:\n",
    "    print(f\"Testing model: {model}\")\n",
    "    evaluator = LLMModel2(\n",
    "        provider=provider,\n",
    "        model=model,\n",
    "        output_schema=NotebookWiseFeedback,\n",
    "        name=\"aspect_evaluator\",\n",
    "        prompt_template=chat_template,\n",
    "        try_to_parse=True,\n",
    "        config={\n",
    "            \"retry\": 3,\n",
    "        \n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Perform evaluation\n",
    "    evaluation_result = evaluator({\n",
    "        \"conversation\": conversation\n",
    "    })\n",
    "    print(evaluation_result)\n",
    "    NotebookWiseFeedback.model_validate(evaluation_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: ...Manual token count\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'in': 2083, 'out': 319},\n",
       " {'in': ['\\nName: AI Assistant Perfector.\\nProfile: You are an expert in perfecting AI Assistants\\' response content based on the user\\'s expertise level.\\nThe output should be formatted as a JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output schema:\\n```\\n{\"$defs\": {\"Intent\": {\"description\": \"User intent.\", \"properties\": {\"cell_pos\": {\"title\": \"Cell Pos\", \"type\": \"integer\"}, \"intent\": {\"description\": \"User intent, concise, single sentence per user reply. Avoid Assistant intent\", \"title\": \"Intent\", \"type\": \"string\"}}, \"required\": [\"cell_pos\", \"intent\"], \"title\": \"Intent\", \"type\": \"object\"}, \"Issue\": {\"description\": \"SINGLE ISSUE\", \"properties\": {\"type\": {\"description\": \"Type of the issue identified.\", \"title\": \"Type\", \"type\": \"string\"}, \"reason\": {\"description\": \"What is the problem, 1-2 short sentences max. Specifically, what went wrong.\", \"title\": \"Reason\", \"type\": \"string\"}, \"cell_positions\": {\"description\": \"List of cell positions that contain the issue.\", \"items\": {\"type\": \"integer\"}, \"title\": \"Cell Positions\", \"type\": \"array\"}, \"fix\": {\"description\": \"Suggested fix for the issue, concise but with a clear guidance, not generic. Provide short example like e.g. ... but not a specific example something in between for reference but not for copy paste. No quoted examples. Just reference to what\\'s needed.\", \"title\": \"Fix\", \"type\": \"string\"}}, \"required\": [\"type\", \"reason\", \"cell_positions\", \"fix\"], \"title\": \"Issue\", \"type\": \"object\"}, \"UserExpertise\": {\"description\": \"User expertise level in a inquiry topic.\", \"properties\": {\"inquiry_topic\": {\"description\": \"The topic of the user\\'s inquiry.\", \"title\": \"Inquiry Topic\", \"type\": \"string\"}, \"level\": {\"description\": \"The user\\'s expertise level in the inquiry topic. Even if assumed. Be concise, 1 sentence max.\", \"title\": \"Level\", \"type\": \"string\"}}, \"required\": [\"inquiry_topic\", \"level\"], \"title\": \"UserExpertise\", \"type\": \"object\"}}, \"description\": \"Scoring of the totality of found issues.\", \"properties\": {\"user_intents\": {\"description\": \"Identify user intents. Avoid Assistant intent.\", \"items\": {\"$ref\": \"#/$defs/Intent\"}, \"title\": \"User Intents\", \"type\": \"array\"}, \"user_expertise_level\": {\"allOf\": [{\"$ref\": \"#/$defs/UserExpertise\"}], \"description\": \"Level of expertise and knowledge of the human User in their inquiry topic.\"}, \"scratchpad\": {\"description\": \"Place for you to think. Think before issues and score creation. Be concise. Analyze the text to achieve your goal. Always think before looking for issues!\", \"title\": \"Scratchpad\", \"type\": \"string\"}, \"issues\": {\"description\": \"A list of issues in the notebook, each described with where, type, and fix.\", \"items\": {\"$ref\": \"#/$defs/Issue\"}, \"title\": \"Issues\", \"type\": \"array\"}, \"score\": {\"description\": \"A score representing how good the conversation is, given the total list of mistakes, 1 is far from perfect, 5 is no issues at all.\", \"maximum\": 5, \"minimum\": 1, \"title\": \"Score\", \"type\": \"integer\"}}, \"required\": [\"user_intents\", \"user_expertise_level\", \"scratchpad\", \"issues\", \"score\"]}\\n```\\nGiven the following conversation between Human User and AI Assistant, find issues following the rules described below and rate the total conversation.\\nSingle significant instance of deviation from the rules - score 1 or 2. More issues score<2. No issues=5.\\n\\nQualities we care about. Focus on them and only find issues that are directly related to them:\\n```\\nYou must assume the user just started to learn about the question that is asked, so the \\nreply should cover all the points that the user might be new to, and assume the \\nuser has basic knowledge about the prerequisites. \\n\\nThis helps us keep the explanation clean, and makes it useful to the user rather \\nthan throwing all information about the topic to the user.\\n\\nIt is important to identify the query intent to gauge the user knowledge level as well as \\nthe code complexity to provide the most useful explanation.\\n```\\n\\nThe task:\\n```\\nPlease, detect all mismatches between user\\'s expertise level shown and the replies of the Assistant.\\nIf User expertise level is unknown - asumme they are a beginner in that question.\\nMismatches might include but not limited to:\\n    - too much explanation for an expert\\n    - too little explanation for a beginner\\n    - Assistant assumes the user is not the beginner in the question asked be it an algo or a technology or something else.\\n\\nAssume basic knowledge of Python programming by the user and so no need to explain basic things unless asked to.\\nFor example, if the question is about an algorithm in python, assume understnding of Python but a beginner level in algorithms UNLESS USER SHOWS OR STATES A HIGHER OR LOWER LEVEL OF EXPERTISE.\\n\\nIf no issues found, do not create any.\\nCorrectness or accuracy is not your concern and will be handled by other evaluators. Focus only on the serving user\\'s level of expertise in the most helpful manner.\\n```\\n\\nConversation:\\nCONVERSATION_START\\n{\\'status\\': \\'OK\\', \\'metadata\\': {\\'metadata\\': \\'# Metadata\\\\n\\\\n**Python Topics** - algorithms > by_topic > probability\\\\n\\\\n**Type** - query\\\\n\\\\n**Target Number of Turns (User + Assistant)** - 2\\\\n\\'}, \\'conversation\\': [{\\'cell_pos\\': 1, \\'role\\': \\'User\\', \\'content\\': \\'Could you show me a Python function to calculate the probability of rolling a sum of 8 with two six-sided dice?\\', \\'type\\': \\'markdown\\'}, {\\'cell_pos\\': 2, \\'role\\': \\'Assistant\\', \\'content\\': \\'Certainly! When you roll two six-sided dice, there are 36 possible outcomes (as there are 6 possible outcomes for each die). To determine the probability of obtaining a sum of 8, you must identify the favorable outcomes (those resulting in a sum of 8) and then divide this count by the total number of possible outcomes.\\\\n\\\\nHere is a python function for it:\\', \\'type\\': \\'markdown\\'}, {\\'cell_pos\\': 3, \\'role\\': \\'Assistant\\', \\'content\\': \\'def probability_of_rolling_sum_eight():\\\\n    favorable_outcomes = 0\\\\n    total_outcomes = 36 # There are 6 * 6 = 36 total outcomes\\\\n\\\\n    for dice_1 in range(1, 7):\\\\n        dice_2 = 8 - dice_1\\\\n\\\\n        # Check if the value is valid\\\\n        if 1 <= dice_2 <= 6:\\\\n            favorable_outcomes += 1\\\\n\\\\n    return favorable_outcomes / total_outcomes\\\\n\\\\n# Call the function and print the result\\\\nprobability_of_rolling_sum_eight = probability_of_rolling_sum_eight()\\\\n\\\\nprint(f\"The probability of rolling a sum of 8 with two six-sided dice is: {probability_of_rolling_sum_eight}\")\\', \\'type\\': \\'code\\'}, {\\'cell_pos\\': 4, \\'role\\': \\'Assistant\\', \\'content\\': \\'The time complexity of the `probability_of_rolling_sum_eight` function is O(N) since it undergoes a constant number of iterations (6 iterations, corresponding to each possible value of dice_1 ranging from 1 to 6).The loop entails basic arithmetic operations and conditional checks, all with constant time complexity.\\\\n\\\\nAs a result, the time complexity remains constant and is not influenced by the input size, irrespective of the number of sides on the dice. The function is specifically tailored for six-sided dice, ensuring a consistent and limited number of iterations.\\', \\'type\\': \\'markdown\\'}, {\\'cell_pos\\': 5, \\'role\\': \\'User\\', \\'content\\': \\'Could you kindly provide the result with precision up to four decimal places only?\\', \\'type\\': \\'markdown\\'}, {\\'cell_pos\\': 6, \\'role\\': \\'Assistant\\', \\'content\\': \"Certainly! Here\\'s the revised version of the function that will provide the outcome with accuracy up to four decimal places.\", \\'type\\': \\'markdown\\'}, {\\'cell_pos\\': 7, \\'role\\': \\'Assistant\\', \\'content\\': \\'def probability_of_rolling_sum_eight():\\\\n    favorable_outcomes = 0\\\\n    total_outcomes = 36 # There are 6 * 6 = 36 total outcomes\\\\n\\\\n    for die1 in range(1, 7):\\\\n        die2 = 8 - die1\\\\n\\\\n        # Check if the value is valid\\\\n        if 1 <= die2 <= 6:\\\\n            favorable_outcomes += 1\\\\n\\\\n    return favorable_outcomes / total_outcomes\\\\n\\\\n# Call the function and print the result\\\\nprobability_of_rolling_sum_eight = probability_of_rolling_sum_eight()\\\\n\\\\nprint(f\"The probability of rolling a sum of 8 with two six-sided dice is: {probability_of_rolling_sum_eight: .4f}\")\\', \\'type\\': \\'code\\'}]}\\nCONVERSATION_END\\n\\nNow, proceed to completing your task of finding issues and scoring the conversation.\\n'],\n",
       "  'out': ['Based on the given conversation, here\\'s the formatted JSON instance:\\n\\n```json\\n{\\n  \"user_intents\": [\\n    {\\n      \"cell_pos\": 1,\\n      \"intent\": \"Request for a Python function to calculate the probability of rolling a sum of 8 with two six-sided dice\"\\n    },\\n    {\\n      \"cell_pos\": 5,\\n      \"intent\": \"Request for the result with precision up to four decimal places\"\\n    }\\n  ],\\n  \"user_expertise_level\": {\\n    \"inquiry_topic\": \"probability of rolling a sum of 8 with two six-sided dice\",\\n    \"level\": \"beginner\"\\n  },\\n  \"scratchpad\": \"The user is a beginner in the topic of probability of rolling a sum of 8 with two six-sided dice. The assistant provided a Python function to calculate the probability and explained the time complexity. The user then asked for the result with precision up to four decimal places.\",\\n  \"issues\": [\\n    {\\n      \"type\": \"Mismatches between user\\'s expertise level and the replies of the Assistant\",\\n      \"reason\": \"The assistant provided a detailed explanation of the time complexity, which might be overwhelming for a beginner.\",\\n      \"cell_positions\": [4],\\n      \"fix\": \"The assistant could have simplified the explanation or provided it as an optional detail.\"\\n    }\\n  ],\\n  \"score\": 2\\n}\\n```\\n\\nThe score is 2 because there is one issue found related to the mismatch between the user\\'s expertise level and the assistant\\'s detailed explanation of the time complexity.']})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.get_total_tokens()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "evaluator",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
