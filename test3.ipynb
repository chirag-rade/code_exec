{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from dotenv import  load_dotenv\n",
    "load_dotenv()\n",
    "from Models.models import LLMModel as LLMModel2\n",
    "import pytest\n",
    "from langchain.prompts import HumanMessagePromptTemplate\n",
    "from langchain_core.messages import (HumanMessage)\n",
    "from langchain.prompts import SystemMessagePromptTemplate, ChatPromptTemplate, MessagesPlaceholder\n",
    "from pydantic import BaseModel, Field, validator\n",
    "import json\n",
    "from Models.schemas import NotebookWiseFeedback\n",
    "\n",
    "\n",
    "\n",
    "#----------------------------------------------------------------\n",
    "class Joke(BaseModel):\n",
    "    \"\"\"Joke model\"\"\"\n",
    "    SETUP: str = Field(..., description=\"setup for the joke\")\n",
    "    PUNCHLINE: str = Field(..., description=\"punchline for the joke\")\n",
    "\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "            [\n",
    "                (\n",
    "                    \"human\",\n",
    "                    \"Given this topic={topic}, generate a joke.\"\n",
    "                ),\n",
    "                (\n",
    "                    \"human\",\n",
    "                    \"Also, make your output all upper case LIKE THIS.\"\n",
    "                )\n",
    "               \n",
    "            ]\n",
    "        )\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "conversation = {'status': 'OK',\n",
    " 'metadata': {'metadata': '# Metadata\\n\\n**Python Topics** - algorithms > by_topic > probability\\n\\n**Type** - query\\n\\n**Target Number of Turns (User + Assistant)** - 2\\n'},\n",
    " 'conversation': [{'cell_pos': 1,\n",
    "   'role': 'User',\n",
    "   'content': 'Could you show me a Python function to calculate the probability of rolling a sum of 8 with two six-sided dice?',\n",
    "   'type': 'markdown'},\n",
    "  {'cell_pos': 2,\n",
    "   'role': 'Assistant',\n",
    "   'content': 'Certainly! When you roll two six-sided dice, there are 36 possible outcomes (as there are 6 possible outcomes for each die). To determine the probability of obtaining a sum of 8, you must identify the favorable outcomes (those resulting in a sum of 8) and then divide this count by the total number of possible outcomes.\\n\\nHere is a python function for it:',\n",
    "   'type': 'markdown'},\n",
    "  {'cell_pos': 3,\n",
    "   'role': 'Assistant',\n",
    "   'content': 'def probability_of_rolling_sum_eight():\\n    favorable_outcomes = 0\\n    total_outcomes = 36 # There are 6 * 6 = 36 total outcomes\\n\\n    for dice_1 in range(1, 7):\\n        dice_2 = 8 - dice_1\\n\\n        # Check if the value is valid\\n        if 1 <= dice_2 <= 6:\\n            favorable_outcomes += 1\\n\\n    return favorable_outcomes / total_outcomes\\n\\n# Call the function and print the result\\nprobability_of_rolling_sum_eight = probability_of_rolling_sum_eight()\\n\\nprint(f\"The probability of rolling a sum of 8 with two six-sided dice is: {probability_of_rolling_sum_eight}\")',\n",
    "   'type': 'code'},\n",
    "  {'cell_pos': 4,\n",
    "   'role': 'Assistant',\n",
    "   'content': 'The time complexity of the `probability_of_rolling_sum_eight` function is O(N) since it undergoes a constant number of iterations (6 iterations, corresponding to each possible value of dice_1 ranging from 1 to 6).The loop entails basic arithmetic operations and conditional checks, all with constant time complexity.\\n\\nAs a result, the time complexity remains constant and is not influenced by the input size, irrespective of the number of sides on the dice. The function is specifically tailored for six-sided dice, ensuring a consistent and limited number of iterations.',\n",
    "   'type': 'markdown'},\n",
    "  {'cell_pos': 5,\n",
    "   'role': 'User',\n",
    "   'content': 'Could you kindly provide the result with precision up to four decimal places only?',\n",
    "   'type': 'markdown'},\n",
    "  {'cell_pos': 6,\n",
    "   'role': 'Assistant',\n",
    "   'content': \"Certainly! Here's the revised version of the function that will provide the outcome with accuracy up to four decimal places.\",\n",
    "   'type': 'markdown'},\n",
    "  {'cell_pos': 7,\n",
    "   'role': 'Assistant',\n",
    "   'content': 'def probability_of_rolling_sum_eight():\\n    favorable_outcomes = 0\\n    total_outcomes = 36 # There are 6 * 6 = 36 total outcomes\\n\\n    for die1 in range(1, 7):\\n        die2 = 8 - die1\\n\\n        # Check if the value is valid\\n        if 1 <= die2 <= 6:\\n            favorable_outcomes += 1\\n\\n    return favorable_outcomes / total_outcomes\\n\\n# Call the function and print the result\\nprobability_of_rolling_sum_eight = probability_of_rolling_sum_eight()\\n\\nprint(f\"The probability of rolling a sum of 8 with two six-sided dice is: {probability_of_rolling_sum_eight: .4f}\")',\n",
    "   'type': 'code'}]}\n",
    "\n",
    "chat_template = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            SystemMessagePromptTemplate.from_template(\n",
    "                \"\"\"Name: AI Assistant Perfector.\n",
    "Profile: You are an expert in perfecting AI Assistants' response content based on the user's expertise level.\"\"\"\n",
    "            ),\n",
    "            HumanMessagePromptTemplate.from_template(\n",
    "                \"\"\"\n",
    "Given the following conversation between Human User and AI Assistant, find issues following the rules described below and rate the total conversation.\n",
    "Single significant instance of deviation from the rules - score 1 or 2. More issues score<2. No issues=5.\n",
    "\n",
    "Qualities we care about. Focus on them and only find issues that are directly related to them:\n",
    "```\n",
    "You must assume the user just started to learn about the question that is asked, so the \n",
    "reply should cover all the points that the user might be new to, and assume the \n",
    "user has basic knowledge about the prerequisites. \n",
    "\n",
    "This helps us keep the explanation clean, and makes it useful to the user rather \n",
    "than throwing all information about the topic to the user.\n",
    "\n",
    "It is important to identify the query intent to gauge the user knowledge level as well as \n",
    "the code complexity to provide the most useful explanation.\n",
    "```\n",
    "\n",
    "The task:\n",
    "```\n",
    "Please, detect all mismatches between user's expertise level shown and the replies of the Assistant.\n",
    "If User expertise level is unknown - asumme they are a beginner in that question.\n",
    "Mismatches might include but not limited to:\n",
    "    - too much explanation for an expert\n",
    "    - too little explanation for a beginner\n",
    "    - Assistant assumes the user is not the beginner in the question asked be it an algo or a technology or something else.\n",
    "\n",
    "Assume basic knowledge of Python programming by the user and so no need to explain basic things unless asked to.\n",
    "For example, if the question is about an algorithm in python, assume understnding of Python but a beginner level in algorithms UNLESS USER SHOWS OR STATES A HIGHER OR LOWER LEVEL OF EXPERTISE.\n",
    "\n",
    "If no issues found, do not create any.\n",
    "Correctness or accuracy is not your concern and will be handled by other evaluators. Focus only on the serving user's level of expertise in the most helpful manner.\n",
    "```\n",
    "\n",
    "Conversation:\n",
    "CONVERSATION_START\n",
    "{conversation}\n",
    "CONVERSATION_END\n",
    "\n",
    "Now, proceed to completing your task of finding issues and scoring the conversation.\n",
    "\"\"\"\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "#----------------------------------------------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_= {\n",
    "                \"conversation\": conversation\n",
    "            }\n",
    "\n",
    "p = chat_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def test_anthropic_models():\n",
    "        models = [\n",
    "            \"claude-3-opus-20240229\",\n",
    "        ]\n",
    "        provider = \"anthropic_api\"\n",
    "        print(f\"Testing models from provider: {provider}\")\n",
    "        for model in models:\n",
    "            print(f\"Testing model: {model}\")\n",
    "            evaluator = LLMModel2(\n",
    "                provider=provider,\n",
    "                model=model,\n",
    "                output_schema=Joke.model_json_schema(),\n",
    "                use_tool=False,\n",
    "                name=\"aspect_evaluator\",\n",
    "                   prompt_template=prompt_template,\n",
    "                try_to_parse=True,\n",
    "                config={\n",
    "                    \"retry\": 3,\n",
    "                    \n",
    "                }\n",
    "            )\n",
    "\n",
    "            # Perform evaluation\n",
    "            evaluation_result = evaluator({\n",
    "                \"topic\": \"dad joke\"\n",
    "            })\n",
    "            print(evaluation_result)\n",
    "            Joke.model_validate(evaluation_result)\n",
    "\n",
    "\n",
    "def test_fireworks_models():\n",
    "        models = [\n",
    "        \"llama-v3-70b-instruct\",\n",
    "    \n",
    "          #  \"dbrx-instruct\",\n",
    "            \"mixtral-8x22b-instruct\",\n",
    "            \"llama-v3-8b-instruct\"\n",
    "        ]\n",
    "        provider = \"fireworks_api\"\n",
    "        print(f\"Testing models from provider: {provider}\")\n",
    "        for model in models:\n",
    "            print(f\"Testing model: {model}\")\n",
    "            evaluator = LLMModel2(\n",
    "                provider=provider,\n",
    "                model=model,\n",
    "                output_schema=Joke.model_json_schema(),\n",
    "                name=\"aspect_evaluator\",\n",
    "                prompt_template=prompt_template,\n",
    "                try_to_parse=True,\n",
    "                config={\n",
    "                    \"retry\": 3,\n",
    "                \n",
    "                }\n",
    "            )\n",
    "\n",
    "            # Perform evaluation\n",
    "            evaluation_result = evaluator({\n",
    "                \"topic\": \"dad joke\"\n",
    "            })\n",
    "            print(evaluation_result)\n",
    "            Joke.model_validate(evaluation_result)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def test_openai_models(): \n",
    "        models = [\n",
    "             \"gpt-3.5-turbo-1106\",\n",
    "            \"gpt-4-turbo\",\n",
    "            \"gpt-3.5-turbo\",\n",
    "            \"gpt-4o\",\n",
    "        ]\n",
    "        provider = \"openai_api\"\n",
    "        print(f\"Testing models from provider: {provider}\")\n",
    "        for model in models:\n",
    "            print(f\"Testing model: {model}\")\n",
    "            evaluator = LLMModel2(\n",
    "                provider=provider,\n",
    "                model=model,\n",
    "                #use_tool=False,\n",
    "                output_schema=Joke.model_json_schema(),\n",
    "                name=\"aspect_evaluator\",\n",
    "                  prompt_template=prompt_template,\n",
    "                try_to_parse=True,\n",
    "                config={\n",
    "                    \"retry\": 4,\n",
    "                }\n",
    "            )\n",
    "\n",
    "            # Perform evaluation\n",
    "            evaluation_result = evaluator({\n",
    "               \"topic\":\"dad joke\"\n",
    "            })\n",
    "            print(evaluation_result)\n",
    "            Joke.model_validate(evaluation_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def test_anthropic_models():\n",
    "        models = [\n",
    "            \"claude-3-opus-20240229\",\n",
    "        ]\n",
    "        provider = \"anthropic_api\"\n",
    "        print(f\"Testing models from provider: {provider}\")\n",
    "        for model in models:\n",
    "            print(f\"Testing model: {model}\")\n",
    "            evaluator = LLMModel2(\n",
    "                provider=provider,\n",
    "                model=model,\n",
    "                output_schema=NotebookWiseFeedback.model_json_schema(),\n",
    "                name=\"aspect_evaluator\",\n",
    "                prompt_template=chat_template,\n",
    "                try_to_parse=True,\n",
    "                config={\n",
    "                    \"retry\": 3,\n",
    "                    \n",
    "                }\n",
    "            )\n",
    "\n",
    "            # Perform evaluation\n",
    "            evaluation_result = evaluator({\n",
    "                \"conversation\": conversation\n",
    "            })\n",
    "            print(evaluation_result)\n",
    "            NotebookWiseFeedback.model_validate(evaluation_result)\n",
    "\n",
    "\n",
    "def test_fireworks_models():\n",
    "        models = [\n",
    "        \"llama-v3-70b-instruct\",\n",
    "            \"dbrx-instruct\",\n",
    "            \"mixtral-8x22b-instruct\",\n",
    "            \"llama-v3-8b-instruct\"\n",
    "        ]\n",
    "        provider = \"fireworks_api\"\n",
    "        print(f\"Testing models from provider: {provider}\")\n",
    "        for model in models:\n",
    "            print(f\"Testing model: {model}\")\n",
    "            evaluator = LLMModel2(\n",
    "                provider=provider,\n",
    "                model=model,\n",
    "                output_schema=NotebookWiseFeedback.model_json_schema(),\n",
    "                name=\"aspect_evaluator\",\n",
    "                prompt_template=chat_template,\n",
    "                try_to_parse=True,\n",
    "                config={\n",
    "                    \"retry\": 3,\n",
    "                \n",
    "                }\n",
    "            )\n",
    "\n",
    "            # Perform evaluation\n",
    "            evaluation_result = evaluator({\n",
    "                \"conversation\": conversation\n",
    "            })\n",
    "            print(evaluation_result)\n",
    "            NotebookWiseFeedback.model_validate(evaluation_result)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def test_openai_models(): \n",
    "        models = [\n",
    "             \"gpt-3.5-turbo-1106\",\n",
    "            \"gpt-4-turbo\",\n",
    "            \"gpt-3.5-turbo\",\n",
    "            \"gpt-4o\",\n",
    "        ]\n",
    "        provider = \"openai_api\"\n",
    "        print(f\"Testing models from provider: {provider}\")\n",
    "        for model in models:\n",
    "            print(f\"Testing model: {model}\")\n",
    "            evaluator = LLMModel2(\n",
    "                provider=provider,\n",
    "                model=model,\n",
    "                output_schema=NotebookWiseFeedback.model_json_schema(),\n",
    "                name=\"aspect_evaluator\",\n",
    "                prompt_template=chat_template,\n",
    "                try_to_parse=True,\n",
    "                config={\n",
    "                    \"retry\": 4,\n",
    "                }\n",
    "            )\n",
    "\n",
    "            # Perform evaluation\n",
    "            evaluation_result = evaluator({\n",
    "                \"conversation\": conversation\n",
    "            })\n",
    "            print(evaluation_result)\n",
    "            NotebookWiseFeedback.model_validate(evaluation_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing models from provider: openai_api\n",
      "Testing model: gpt-3.5-turbo-1106\n",
      "{'user_intents': [{'cell_pos': 1, 'intent': 'Request for Python function to calculate the probability of rolling a sum of 8 with two six-sided dice.'}, {'cell_pos': 5, 'intent': 'Request for the result with precision up to four decimal places only.'}], 'user_expertise_level': {'inquiry_topic': 'algorithms', 'level': 'beginner'}, 'scratchpad': 'The user has inquired about calculating the probability of rolling a sum of 8 with two six-sided dice and has requested the result with precision up to four decimal places.', 'issues': [{'type': 'Explanation Mismatch', 'reason': 'The explanation provided assumes a high level of understanding of probability concepts and Python programming.', 'cell_positions': [2, 3, 4, 6, 7], 'fix': 'The explanation should start with a basic introduction to probability and then gradually move on to the specific calculation in Python. Additionally, the Python code provided should be simplified and explained in a beginner-friendly manner.'}, {'type': 'Assumption of User Expertise', 'reason': \"The Assistant assumes a higher level of expertise in probability and Python programming than the user's stated level of beginner.\", 'cell_positions': [2, 3, 4, 6, 7], 'fix': 'The Assistant should assume a beginner level of expertise and provide explanations and code examples accordingly.'}], 'score': 1}\n",
      "Testing model: gpt-4-turbo\n",
      "{'user_intents': [{'cell_pos': 1, 'intent': 'Request a Python function to calculate the probability of a specific sum with two dice.'}, {'cell_pos': 5, 'intent': 'Request to modify the function to provide results with four decimal precision.'}], 'user_expertise_level': {'inquiry_topic': 'Python programming for probability calculations', 'level': 'beginner with basic Python knowledge'}, 'scratchpad': \"The user begins by asking for a Python function to calculate a specific probability, indicating a beginner level in probability calculations but basic Python knowledge. The assistant provides a detailed explanation and a function, which aligns with the user's stated level. However, the explanation about time complexity might be too advanced for a beginner in this topic. The assistant's response to the precision request is straightforward and appropriate.\", 'issues': [{'type': 'Explanation Depth', 'reason': 'The explanation of time complexity (O(N)) is too advanced for a beginner, which may not be necessary for the user who just started learning about this specific calculation.', 'cell_positions': [4], 'fix': 'Simplify the explanation by omitting the time complexity details or briefly mention it without going into depth, focusing more on the practical application of the function.'}], 'score': 4}\n",
      "Testing model: gpt-3.5-turbo\n",
      "{'user_intents': [{'cell_pos': 1, 'intent': 'Requesting Python function to calculate probability of rolling a sum of 8 with two six-sided dice'}, {'cell_pos': 5, 'intent': 'Requesting result with precision up to four decimal places'}], 'user_expertise_level': {'inquiry_topic': 'calculating probability of rolling a sum with dice', 'level': 'beginner'}, 'scratchpad': 'The user is inquiring about calculating the probability of rolling a sum with dice and requested a Python function for it with precision up to four decimal places.', 'issues': [{'type': 'Explanation Mismatch', 'reason': 'The Assistant provides a detailed explanation and code complexity information that may be overwhelming for a beginner level user inquiring about a simple probability calculation.', 'cell_positions': [2, 4], 'fix': 'Simplify the explanation by focusing on the basic steps of calculating the probability without delving into unnecessary complexity. Provide a straightforward Python function without additional information about time complexity.'}, {'type': 'Assumed Expertise Level', 'reason': 'The Assistant assumes a higher level of expertise by discussing time complexity and tailored functions, which may not be relevant or understandable for a beginner inquiring about basic probability calculations.', 'cell_positions': [4], 'fix': 'Avoid discussing advanced concepts like time complexity and tailored functions. Stick to the basics and provide a simple Python function for calculating the probability.'}], 'score': 2}\n",
      "Testing model: gpt-4o\n",
      "{'user_intents': [{'cell_pos': 1, 'intent': 'Ask for a Python function to calculate the probability of rolling a sum of 8 with two six-sided dice'}, {'cell_pos': 5, 'intent': 'Ask for the probability result with precision up to four decimal places'}], 'user_expertise_level': {'inquiry_topic': 'probability calculation with dice in Python', 'level': 'beginner'}, 'scratchpad': \"The user is asking about a basic probability calculation of rolling a sum of 8 with two six-sided dice in Python. The user's expertise level is assumed to be beginner.\\n\\nCell 2 provides a clear explanation suitable for a beginner, explaining the total possible outcomes and favorable outcomes.\\n\\nCell 3 provides a Python function, which is appropriate for a user with basic Python knowledge.\\n\\nCell 4 discusses the time complexity of the function, which might be too advanced for a beginner in this context. Beginners in probability calculations with dice might not need to know about time complexity, especially since the problem is simple and the user did not ask for this information.\\n\\nCell 6 is a good response to the user's request for precision, and Cell 7 provides the revised function that meets the user's request for four decimal places.\", 'issues': [{'type': 'excessive_information', 'reason': 'The explanation of time complexity in Cell 4 is too advanced for a beginner user and was not asked for.', 'cell_positions': [4], 'fix': 'Remove the explanation of time complexity, as it is not necessary for understanding the basic probability calculation.'}], 'score': 4}\n"
     ]
    }
   ],
   "source": [
    "test_openai_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing models from provider: anthropic_api\n",
      "Testing model: claude-3-opus-20240229\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/daniel/anaconda3/envs/evaluator/lib/python3.12/site-packages/langchain_core/_api/beta_decorator.py:87: LangChainBetaWarning: The method `ChatAnthropic.bind_tools` is in beta. It is actively being worked on, so the API may change.\n",
      "  warn_beta(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'user_intents': [{'cell_pos': 1, 'intent': 'Calculate probability of rolling sum of 8 with two six-sided dice in Python'}, {'cell_pos': 5, 'intent': 'Print the probability result with 4 decimal places precision'}], 'user_expertise_level': {'inquiry_topic': 'calculating probability in Python', 'level': 'beginner'}, 'scratchpad': \"The user is a beginner in calculating probability. The assistant provides a detailed explanation and code example which is good for a beginner. However, it also explains time complexity which may be too advanced unless the user asked for it. The assistant handles the user's request for 4 decimal place precision well.\", 'issues': [{'type': 'Extra information', 'reason': 'The assistant explains the time complexity of the solution, which may be too advanced for a beginner unless they specifically asked about it.', 'cell_positions': [4], 'fix': 'Only explain time complexity if the user asks about it or shows interest in more advanced topics. Otherwise keep explanations focused on the core concepts needed to answer the question for a beginner.'}], 'score': 4}\n"
     ]
    }
   ],
   "source": [
    "test_anthropic_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing models from provider: fireworks_api\n",
      "Testing model: llama-v3-70b-instruct\n",
      "{'user_intents': [{'cell_pos': 1, 'intent': 'Calculate the probability of rolling a sum of 8 with two six-sided dice'}, {'cell_pos': 5, 'intent': 'Provide the result with precision up to four decimal places'}], 'user_expertise_level': {'inquiry_topic': 'algorithms', 'level': 'beginner'}, 'scratchpad': \"Analyzing the conversation to identify mismatches between user's expertise level and the Assistant's replies.\", 'issues': [{'type': 'too much explanation', 'reason': 'The Assistant provided an explanation of the time complexity of the function, which might be too advanced for a beginner in algorithms.', 'cell_positions': [4], 'fix': 'Provide a more concise explanation or skip the time complexity explanation for a beginner.'}], 'score': 4}\n",
      "Testing model: dbrx-instruct\n",
      "{'user_intents': [{'cell_pos': 1, 'intent': 'request_help'}, {'cell_pos': 5, 'intent': 'request_help'}], 'user_expertise_level': {'inquiry_topic': 'probability', 'level': 'beginner'}, 'scratchpad': 'The user is a beginner in the topic of probability and has asked for help with calculating the probability of rolling a sum of 8 with two six-sided dice. The assistant has provided a Python function to calculate the probability and has explained the time complexity of the function. The user has requested the result with precision up to four decimal places, and the assistant has provided a revised version of the function to meet this request.', 'issues': [{'type': 'information_excess', 'reason': 'The assistant has provided a detailed explanation of the time complexity of the function, which may not be necessary for a beginner user.', 'cell_positions': [4], 'fix': 'The assistant could have simplified the explanation of the time complexity for a beginner user, or omitted it altogether.'}, {'type': 'information_lack', 'reason': 'The assistant has not explicitly stated that the function assumes two six-sided dice, which may not be clear to a beginner user.', 'cell_positions': [2, 3, 7], 'fix': 'The assistant could have explicitly stated that the function is for two six-sided dice in the explanation and the code comments.'}], 'score': 3}\n",
      "Testing model: mixtral-8x22b-instruct\n",
      "{'user_intents': [{'cell_pos': 1, 'intent': 'request-solution-probability-of-rolling-sum-eight-with-two-dice'}, {'cell_pos': 5, 'intent': 'request-result-with-precision'}], 'user_expertise_level': {'inquiry_topic': 'probability-and-python', 'level': 'beginner'}, 'scratchpad': 'The user is a beginner in probability and asking for a solution to calculate the probability of rolling a sum of 8 with two six-sided dice. The user then asks for the result with precision up to four decimal places.', 'issues': [{'type': 'too-much-explanation', 'reason': \"The assistant provides an unnecessary explanation of time complexity for the function. Although the user has basic knowledge of Python, they are a beginner in probability and did not ask for an analysis of the function's time complexity.\", 'cell_positions': [4], 'fix': 'Remove the explanation of time complexity.'}], 'score': 4}\n",
      "Testing model: llama-v3-8b-instruct\n",
      "{'user_intents': [{'cell_pos': 1, 'intent': 'query'}], 'user_expertise_level': {'inquiry_topic': 'algorithms', 'level': 'beginner'}, 'scratchpad': 'The user is a beginner in algorithms, so the assistant should provide a clear explanation of the concept and the code implementation.', 'issues': [{'type': 'mismatch', 'reason': 'The assistant assumes the user has knowledge of Python programming, but this is not explicitly stated in the conversation.', 'cell_positions': [1], 'fix': \"The assistant should consider the user's expertise level and provide a more detailed explanation of the code implementation.\"}, {'type': 'mismatch', 'reason': 'The assistant provides a detailed explanation of the code implementation, which is suitable for a beginner but may be too much for an expert.', 'cell_positions': [2, 4], 'fix': \"The assistant should consider the user's expertise level and provide a more concise explanation of the code implementation.\"}, {'type': 'mismatch', 'reason': 'The assistant does not provide a clear explanation of the code implementation for the revised function.', 'cell_positions': [6], 'fix': 'The assistant should provide a clear explanation of the code implementation for the revised function.'}], 'score': 2}\n"
     ]
    }
   ],
   "source": [
    "test_fireworks_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'$defs': {'Intent': {'description': 'User intent.',\n",
       "   'properties': {'cell_pos': {'title': 'Cell Pos', 'type': 'integer'},\n",
       "    'intent': {'description': 'User intent, concise, single sentence per user reply. Avoid Assistant intent',\n",
       "     'title': 'Intent',\n",
       "     'type': 'string'}},\n",
       "   'required': ['cell_pos', 'intent'],\n",
       "   'title': 'Intent',\n",
       "   'type': 'object'},\n",
       "  'Issue': {'description': 'SINGLE ISSUE',\n",
       "   'properties': {'type': {'description': 'Type of the issue identified.',\n",
       "     'title': 'Type',\n",
       "     'type': 'string'},\n",
       "    'reason': {'description': 'What is the problem, 1-2 short sentences max. Specifically, what went wrong.',\n",
       "     'title': 'Reason',\n",
       "     'type': 'string'},\n",
       "    'cell_positions': {'description': 'List of cell positions that contain the issue.',\n",
       "     'items': {'type': 'integer'},\n",
       "     'title': 'Cell Positions',\n",
       "     'type': 'array'},\n",
       "    'fix': {'description': \"Suggested fix for the issue, concise but with a clear guidance, not generic. Provide short example like e.g. ... but not a specific example something in between for reference but not for copy paste. No quoted examples. Just reference to what's needed.\",\n",
       "     'title': 'Fix',\n",
       "     'type': 'string'}},\n",
       "   'required': ['type', 'reason', 'cell_positions', 'fix'],\n",
       "   'title': 'Issue',\n",
       "   'type': 'object'},\n",
       "  'UserExpertise': {'description': 'User expertise level in a inquiry topic.',\n",
       "   'properties': {'inquiry_topic': {'description': \"The topic of the user's inquiry.\",\n",
       "     'title': 'Inquiry Topic',\n",
       "     'type': 'string'},\n",
       "    'level': {'description': \"The user's expertise level in the inquiry topic. Even if assumed. Be concise, 1 sentence max.\",\n",
       "     'title': 'Level',\n",
       "     'type': 'string'}},\n",
       "   'required': ['inquiry_topic', 'level'],\n",
       "   'title': 'UserExpertise',\n",
       "   'type': 'object'}},\n",
       " 'description': 'Scoring of the totality of found issues.',\n",
       " 'properties': {'user_intents': {'description': 'Identify user intents. Avoid Assistant intent.',\n",
       "   'items': {'$ref': '#/$defs/Intent'},\n",
       "   'title': 'User Intents',\n",
       "   'type': 'array'},\n",
       "  'user_expertise_level': {'allOf': [{'$ref': '#/$defs/UserExpertise'}],\n",
       "   'description': 'Level of expertise and knowledge of the human User in their inquiry topic.'},\n",
       "  'scratchpad': {'description': 'Place for you to think. Think before issues and score creation. Be concise. Analyze the text to achieve your goal. Always think before looking for issues!',\n",
       "   'title': 'Scratchpad',\n",
       "   'type': 'string'},\n",
       "  'issues': {'description': 'A list of issues in the notebook, each described with where, type, and fix.',\n",
       "   'items': {'$ref': '#/$defs/Issue'},\n",
       "   'title': 'Issues',\n",
       "   'type': 'array'},\n",
       "  'score': {'description': 'A score representing how good the conversation is, given the total list of mistakes, 1 is far from perfect, 5 is no issues at all.',\n",
       "   'maximum': 5,\n",
       "   'minimum': 1,\n",
       "   'title': 'Score',\n",
       "   'type': 'integer'}},\n",
       " 'required': ['user_intents',\n",
       "  'user_expertise_level',\n",
       "  'scratchpad',\n",
       "  'issues',\n",
       "  'score'],\n",
       " 'title': 'NotebookWiseFeedback',\n",
       " 'type': 'object'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from Models import schemas\n",
    "schemas.NotebookWiseFeedback.model_json_schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'a' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43ma\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mproperties\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'a' is not defined"
     ]
    }
   ],
   "source": [
    "a[\"properties\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Joke(BaseModel):\n",
    "    \"\"\"Joke model\"\"\"\n",
    "    SETUP: str = Field(..., description=\"setup for the joke\")\n",
    "    PUNCHLINE: str = Field(..., description=\"punchline for the joke\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NotebookWiseFeedback.model_json_schema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def generate_prompt_from_schema(schema):\n",
    "    properties = schema['properties']\n",
    "    fields_description = []\n",
    "    example_json = {}\n",
    "\n",
    "    for field, details in properties.items():\n",
    "        field_type = details['type']\n",
    "        # Adjust the example based on the type\n",
    "        if field_type == 'string':\n",
    "            example_value = f\"Example {field.lower()}\"\n",
    "        elif field_type == 'integer':\n",
    "            example_value = 123\n",
    "        elif field_type == 'boolean':\n",
    "            example_value = True\n",
    "        else:\n",
    "            example_value = f\"Example {field_type}\"\n",
    "\n",
    "        fields_description.append(f\"'{field}': a {field_type}\")\n",
    "        example_json[field] = example_value\n",
    "\n",
    "    fields_description_str = ', '.join(fields_description)\n",
    "    example = json.dumps(example_json, indent=2)\n",
    "    \n",
    "    prompt = (\n",
    "        f\"Generate a JSON object with the following fields: {fields_description_str}. \"\n",
    "        f\"For example, {example}\"\n",
    "    )\n",
    "    return prompt\n",
    "\n",
    "\n",
    "# Assuming you have the schema from Joke.model_json_schema()\n",
    "schema = Joke.model_json_schema()\n",
    "prompt = generate_prompt_from_schema(schema)\n",
    "\n",
    "# Use this prompt in your LLMModel configuration\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", prompt),\n",
    "        (\"human\", \"Please ensure the output is exactly in the format specified.\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generate_prompt_from_schema(NotebookWiseFeedback.model_json_schema()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from dotenv import  load_dotenv\n",
    "load_dotenv()\n",
    "from Models.models import LLMModel \n",
    "import pytest\n",
    "from langchain.prompts import HumanMessagePromptTemplate\n",
    "from langchain_core.messages import (HumanMessage)\n",
    "from langchain.prompts import SystemMessagePromptTemplate, ChatPromptTemplate, MessagesPlaceholder\n",
    "from pydantic import BaseModel, Field, validator\n",
    "import json\n",
    "from Models.schemas import NotebookWiseFeedback\n",
    "\n",
    "\n",
    "\n",
    "conversation = {'status': 'OK',\n",
    " 'metadata': {'metadata': '# Metadata\\n\\n**Python Topics** - algorithms > by_topic > probability\\n\\n**Type** - query\\n\\n**Target Number of Turns (User + Assistant)** - 2\\n'},\n",
    " 'conversation': [{'cell_pos': 1,\n",
    "   'role': 'User',\n",
    "   'content': 'Could you show me a Python function to calculate the probability of rolling a sum of 8 with two six-sided dice?',\n",
    "   'type': 'markdown'},\n",
    "  {'cell_pos': 2,\n",
    "   'role': 'Assistant',\n",
    "   'content': 'Certainly! When you roll two six-sided dice, there are 36 possible outcomes (as there are 6 possible outcomes for each die). To determine the probability of obtaining a sum of 8, you must identify the favorable outcomes (those resulting in a sum of 8) and then divide this count by the total number of possible outcomes.\\n\\nHere is a python function for it:',\n",
    "   'type': 'markdown'},\n",
    "  {'cell_pos': 3,\n",
    "   'role': 'Assistant',\n",
    "   'content': 'def probability_of_rolling_sum_eight():\\n    favorable_outcomes = 0\\n    total_outcomes = 36 # There are 6 * 6 = 36 total outcomes\\n\\n    for dice_1 in range(1, 7):\\n        dice_2 = 8 - dice_1\\n\\n        # Check if the value is valid\\n        if 1 <= dice_2 <= 6:\\n            favorable_outcomes += 1\\n\\n    return favorable_outcomes / total_outcomes\\n\\n# Call the function and print the result\\nprobability_of_rolling_sum_eight = probability_of_rolling_sum_eight()\\n\\nprint(f\"The probability of rolling a sum of 8 with two six-sided dice is: {probability_of_rolling_sum_eight}\")',\n",
    "   'type': 'code'},\n",
    "  {'cell_pos': 4,\n",
    "   'role': 'Assistant',\n",
    "   'content': 'The time complexity of the `probability_of_rolling_sum_eight` function is O(N) since it undergoes a constant number of iterations (6 iterations, corresponding to each possible value of dice_1 ranging from 1 to 6).The loop entails basic arithmetic operations and conditional checks, all with constant time complexity.\\n\\nAs a result, the time complexity remains constant and is not influenced by the input size, irrespective of the number of sides on the dice. The function is specifically tailored for six-sided dice, ensuring a consistent and limited number of iterations.',\n",
    "   'type': 'markdown'},\n",
    "  {'cell_pos': 5,\n",
    "   'role': 'User',\n",
    "   'content': 'Could you kindly provide the result with precision up to four decimal places only?',\n",
    "   'type': 'markdown'},\n",
    "  {'cell_pos': 6,\n",
    "   'role': 'Assistant',\n",
    "   'content': \"Certainly! Here's the revised version of the function that will provide the outcome with accuracy up to four decimal places.\",\n",
    "   'type': 'markdown'},\n",
    "  {'cell_pos': 7,\n",
    "   'role': 'Assistant',\n",
    "   'content': 'def probability_of_rolling_sum_eight():\\n    favorable_outcomes = 0\\n    total_outcomes = 36 # There are 6 * 6 = 36 total outcomes\\n\\n    for die1 in range(1, 7):\\n        die2 = 8 - die1\\n\\n        # Check if the value is valid\\n        if 1 <= die2 <= 6:\\n            favorable_outcomes += 1\\n\\n    return favorable_outcomes / total_outcomes\\n\\n# Call the function and print the result\\nprobability_of_rolling_sum_eight = probability_of_rolling_sum_eight()\\n\\nprint(f\"The probability of rolling a sum of 8 with two six-sided dice is: {probability_of_rolling_sum_eight: .4f}\")',\n",
    "   'type': 'code'}]}\n",
    "\n",
    "chat_template = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            SystemMessagePromptTemplate.from_template(\n",
    "                \"\"\"Name: AI Assistant Perfector.\n",
    "Profile: You are an expert in perfecting AI Assistants' response content based on the user's expertise level.\"\"\"\n",
    "            ),\n",
    "            HumanMessagePromptTemplate.from_template(\n",
    "                \"\"\"\n",
    "Given the following conversation between Human User and AI Assistant, find issues following the rules described below and rate the total conversation.\n",
    "Single significant instance of deviation from the rules - score 1 or 2. More issues score<2. No issues=5.\n",
    "\n",
    "Qualities we care about. Focus on them and only find issues that are directly related to them:\n",
    "```\n",
    "You must assume the user just started to learn about the question that is asked, so the \n",
    "reply should cover all the points that the user might be new to, and assume the \n",
    "user has basic knowledge about the prerequisites. \n",
    "\n",
    "This helps us keep the explanation clean, and makes it useful to the user rather \n",
    "than throwing all information about the topic to the user.\n",
    "\n",
    "It is important to identify the query intent to gauge the user knowledge level as well as \n",
    "the code complexity to provide the most useful explanation.\n",
    "```\n",
    "\n",
    "The task:\n",
    "```\n",
    "Please, detect all mismatches between user's expertise level shown and the replies of the Assistant.\n",
    "If User expertise level is unknown - asumme they are a beginner in that question.\n",
    "Mismatches might include but not limited to:\n",
    "    - too much explanation for an expert\n",
    "    - too little explanation for a beginner\n",
    "    - Assistant assumes the user is not the beginner in the question asked be it an algo or a technology or something else.\n",
    "\n",
    "Assume basic knowledge of Python programming by the user and so no need to explain basic things unless asked to.\n",
    "For example, if the question is about an algorithm in python, assume understnding of Python but a beginner level in algorithms UNLESS USER SHOWS OR STATES A HIGHER OR LOWER LEVEL OF EXPERTISE.\n",
    "\n",
    "If no issues found, do not create any.\n",
    "Correctness or accuracy is not your concern and will be handled by other evaluators. Focus only on the serving user's level of expertise in the most helpful manner.\n",
    "```\n",
    "\n",
    "Conversation:\n",
    "CONVERSATION_START\n",
    "{conversation}\n",
    "CONVERSATION_END\n",
    "\n",
    "Now, proceed to completing your task of finding issues and scoring the conversation.\n",
    "\"\"\"\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "#----------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "class Joke(BaseModel):\n",
    "    \"\"\"Joke model\"\"\"\n",
    "    SETUP: str = Field(..., description=\"setup for the joke\")\n",
    "    PUNCHLINE: str = Field(..., description=\"punchline for the joke\")\n",
    "\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "            [\n",
    "                (\n",
    "                    \"human\",\n",
    "                    \"Given this topic={topic}, generate a joke.\"\n",
    "                ),\n",
    "                (\n",
    "                    \"human\",\n",
    "                    \"Generate the joke\"\n",
    "                )\n",
    "               \n",
    "            ]\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "models = [\"claude-3-opus-20240229\"]     #[\"gpt-3.5-turbo-1106\"] #\n",
    "provider =\"anthropic_api\" #\"openai_api\" #\n",
    "print(f\"Testing models from provider: {provider}\")\n",
    "for model in models:\n",
    "    print(f\"Testing model: {model}\")\n",
    "    evaluator = LLMModel(\n",
    "        provider=provider,\n",
    "        model=model,\n",
    "        output_schema=NotebookWiseFeedback,\n",
    "        name=\"aspect_evaluator\",\n",
    "        prompt_template=chat_template,\n",
    "        try_to_parse=True,\n",
    "        config={\n",
    "            \"retry\": 1,\n",
    "            \n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Perform evaluation\n",
    "    evaluation_result = evaluator({\n",
    "                \"conversation\": conversation\n",
    "            })\n",
    "   \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'$defs': {'Intent': {'description': 'User intent.',\n",
       "   'properties': {'cell_pos': {'title': 'Cell Pos', 'type': 'integer'},\n",
       "    'intent': {'description': 'User intent, concise, single sentence per user reply. Avoid Assistant intent',\n",
       "     'title': 'Intent',\n",
       "     'type': 'string'}},\n",
       "   'required': ['cell_pos', 'intent'],\n",
       "   'title': 'Intent',\n",
       "   'type': 'object'},\n",
       "  'Issue': {'description': 'SINGLE ISSUE',\n",
       "   'properties': {'type': {'description': 'Type of the issue identified.',\n",
       "     'title': 'Type',\n",
       "     'type': 'string'},\n",
       "    'reason': {'description': 'What is the problem, 1-2 short sentences max. Specifically, what went wrong.',\n",
       "     'title': 'Reason',\n",
       "     'type': 'string'},\n",
       "    'cell_positions': {'description': 'List of cell positions that contain the issue.',\n",
       "     'items': {'type': 'integer'},\n",
       "     'title': 'Cell Positions',\n",
       "     'type': 'array'},\n",
       "    'fix': {'description': \"Suggested fix for the issue, concise but with a clear guidance, not generic. Provide short example like e.g. ... but not a specific example something in between for reference but not for copy paste. No quoted examples. Just reference to what's needed.\",\n",
       "     'title': 'Fix',\n",
       "     'type': 'string'}},\n",
       "   'required': ['type', 'reason', 'cell_positions', 'fix'],\n",
       "   'title': 'Issue',\n",
       "   'type': 'object'},\n",
       "  'UserExpertise': {'description': 'User expertise level in a inquiry topic.',\n",
       "   'properties': {'inquiry_topic': {'description': \"The topic of the user's inquiry.\",\n",
       "     'title': 'Inquiry Topic',\n",
       "     'type': 'string'},\n",
       "    'level': {'description': \"The user's expertise level in the inquiry topic. Even if assumed. Be concise, 1 sentence max.\",\n",
       "     'title': 'Level',\n",
       "     'type': 'string'}},\n",
       "   'required': ['inquiry_topic', 'level'],\n",
       "   'title': 'UserExpertise',\n",
       "   'type': 'object'}},\n",
       " 'description': 'Scoring of the totality of found issues.',\n",
       " 'properties': {'user_intents': {'description': 'Identify user intents. Avoid Assistant intent.',\n",
       "   'items': {'$ref': '#/$defs/Intent'},\n",
       "   'title': 'User Intents',\n",
       "   'type': 'array'},\n",
       "  'user_expertise_level': {'allOf': [{'$ref': '#/$defs/UserExpertise'}],\n",
       "   'description': 'Level of expertise and knowledge of the human User in their inquiry topic.'},\n",
       "  'scratchpad': {'description': 'Place for you to think. Think before issues and score creation. Be concise. Analyze the text to achieve your goal. Always think before looking for issues!',\n",
       "   'title': 'Scratchpad',\n",
       "   'type': 'string'},\n",
       "  'issues': {'description': 'A list of issues in the notebook, each described with where, type, and fix.',\n",
       "   'items': {'$ref': '#/$defs/Issue'},\n",
       "   'title': 'Issues',\n",
       "   'type': 'array'},\n",
       "  'score': {'description': 'A score representing how good the conversation is, given the total list of mistakes, 1 is far from perfect, 5 is no issues at all.',\n",
       "   'maximum': 5,\n",
       "   'minimum': 1,\n",
       "   'title': 'Score',\n",
       "   'type': 'integer'}},\n",
       " 'required': ['user_intents',\n",
       "  'user_expertise_level',\n",
       "  'scratchpad',\n",
       "  'issues',\n",
       "  'score'],\n",
       " 'title': 'NotebookWiseFeedback',\n",
       " 'type': 'object'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "NotebookWiseFeedback.model_json_schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "evaluator",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
