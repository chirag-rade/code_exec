{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Models.models import LLMModel\n",
    "from langchain_core.messages import (HumanMessage)\n",
    "from langchain.prompts import SystemMessagePromptTemplate, ChatPromptTemplate, MessagesPlaceholder\n",
    "from pydantic import BaseModel, Field, validator\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a simple callable model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As of my knowledge cutoff in 2023, the President of Nigeria is Muhammadu Buhari. He has been in office since May 29, 2015. However, if you are asking this question at a later date, the officeholder might have changed, particularly after elections or due to other political developments. It is advisable to check the latest information from a reliable source for the current president.\n",
      "--------------------------------\n",
      "As of my knowledge cutoff date in early 2023, the President of Nigeria is Muhammadu Buhari. He has been serving as President since May 29, 2015, after winning the general election. However, given that Nigeria holds presidential elections every four years, and the next election is due in 2023, there may be a new President if you are asking this question after the elections have taken place. Please verify with the most recent sources to get the current President of Nigeria.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "model = LLMModel(\n",
    "    provider=\"openai_api\",\n",
    "    model=\"gpt-4-1106-preview\",\n",
    "    output_schema=None, \n",
    "    )\n",
    "\n",
    "\n",
    "# YOU CAN EITHER PASS A LIST OF LANGCHAIN CORE MESSAGES OR A DICTIONARY WITH KEY `MESSAGES`\n",
    "response = model(\n",
    "    [HumanMessage(content=\"Who is the president of Nigeria\")]\n",
    "    )\n",
    "print(response)\n",
    "\n",
    "print(\"--------------------------------\")\n",
    "\n",
    "response = model(\n",
    "    {\n",
    "        \"messages\": [HumanMessage(content=\"Who is the president of Nigeria\")]\n",
    "    }\n",
    "    )\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Output Schema and prompt template\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating output schema.....\n",
      "{'setup': 'Why did the scarecrow win an award?', 'punchline': 'Because he was outstanding in his field!'}\n",
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "class Joke(BaseModel):\n",
    "    \"\"\"Joke model\"\"\"\n",
    "    setup: str = Field(..., description=\"setup for the joke\")\n",
    "    punchline: str = Field(..., description=\"punchline for the joke\")\n",
    "\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "            [\n",
    "                (\n",
    "                    \"system\",\n",
    "                    \"Given this topic={topic}, generate a joke.\"\n",
    "                )\n",
    "               \n",
    "            ]\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "model1 = LLMModel(\n",
    "    provider=\"fireworks_api\",\n",
    "    model=\"llama-v3-70b-instruct\",\n",
    "    output_schema=Joke, #  This will allow the output to be validated but you can use or Joke.model_json_schema() instead\n",
    "    try_to_parse = True, # model will return JSON object based on the output schema\n",
    "    config = {\"retry\": 2, \"retry_with_history\": True}, #if model fails to output parsable response, it retries 2 time without providing failed response as feedback\n",
    "    prompt_template=prompt_template\n",
    "    )\n",
    "\n",
    "\n",
    "inputs = {\"topic\": \"dad jokes\"}\n",
    "response = model1(inputs)\n",
    "    \n",
    "print(response)\n",
    "print(type(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Extra Model configuration (Parameters and Model Kwargs)\n",
    "\n",
    "You can add parameter such as:\n",
    "\n",
    "1. **retry:** if try_to_parse is set yo true, the module will retry when parsing fails, this value determines how many times it will retry\n",
    "2. **retry_with_history:** if true , the previous AI response will be added to the chat while retrying to improve response\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating output schema.....\n",
      "{'response': \"It doesn't matter what you call him, he's still going to be your loyal, adorable tripod!\"}\n",
      "--------------------------------\n",
      "Validating output schema.....\n",
      "{'response': \"Call him whatever you like, but don't be surprised if he doesn't come running. He's probably too busy being a little bipawd hero!\"}\n",
      "--------------------------------\n",
      "Validating output schema.....\n",
      "{'response': \"A one-legged dog? That sounds like a rough situation, but let's call him 'Uno' – because he's one of a kind and still 'number one' in your heart!\"}\n"
     ]
    }
   ],
   "source": [
    "from Models.schemas import AspectorEvaluatorInputSchema, FeedbackISC, FeedbackBasic,  QualityAspect, AspectorRole\n",
    "\n",
    "model = LLMModel(\n",
    "    provider=\"openai_api\",\n",
    "    model=\"gpt-4-1106-preview\",\n",
    "    output_schema=FeedbackBasic,\n",
    "    prompt_template=ChatPromptTemplate.from_messages([(\"system\", \"You are a funny virtual assistant\")]), #add extra personality configuration\n",
    "    try_to_parse = True, \n",
    "    config = {\"retry\": 2, \n",
    "              \"retry_with_history\": True,\n",
    "              \"params\": {'temperature':0.9}, # Extra model parameters goes here\n",
    "              #\"model_kwargs\": {}  # Extra kwargs can be added here\n",
    "              }\n",
    "    )\n",
    "\n",
    "\n",
    "messages=[HumanMessage(content=\"What do you call a dog with 3 legs?\"),]\n",
    "response = model(messages)\n",
    "print(response)\n",
    "\n",
    "print(\"--------------------------------\")\n",
    "\n",
    "messages=[HumanMessage(content=\"What about  a dog with 2 legs?\"),]\n",
    "response = model(messages)\n",
    "print(response)\n",
    "\n",
    "print(\"--------------------------------\")\n",
    "\n",
    "messages=[HumanMessage(content=\"1 leg?\"),]\n",
    "response = model(messages)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CHAT HISTORY AND TOKEN COUNT\n",
    "1. use `model.chat_history`  to retrieve chat history without system prompt (or prompt template)\n",
    "2. use `model.get_chat_history()` to retrieve chat history with system prompt (or prompt template)\n",
    "3. use `model.chat_history_untouched` to retrieve chat history with system prompt and failed responses (failed parsed responses)\n",
    "4. use `model.get_total_tokens()` to retrieve  both token count and content used to retrieve the count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='What do you call a dog with 3 legs?'),\n",
       " AIMessage(content='```json\\n{\\n  \"response\": \"It doesn\\'t matter what you call him, he\\'s still going to be your loyal, adorable tripod!\"\\n}\\n```', response_metadata={'token_usage': {'completion_tokens': 32, 'prompt_tokens': 186, 'total_tokens': 218}, 'model_name': 'gpt-4-1106-preview', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-c04ce758-129d-44fb-91c9-4bcd5b4c5684-0'),\n",
       " HumanMessage(content='What about  a dog with 2 legs?'),\n",
       " AIMessage(content='```json\\n{\\n  \"response\": \"Call him whatever you like, but don\\'t be surprised if he doesn\\'t come running. He\\'s probably too busy being a little bipawd hero!\"\\n}\\n```', response_metadata={'token_usage': {'completion_tokens': 42, 'prompt_tokens': 236, 'total_tokens': 278}, 'model_name': 'gpt-4-1106-preview', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-e3f78c32-1b56-47cc-8df2-4d63e19e5eb2-0'),\n",
       " HumanMessage(content='1 leg?'),\n",
       " AIMessage(content='```json\\n{\\n  \"response\": \"A one-legged dog? That sounds like a rough situation, but let\\'s call him \\'Uno\\' – because he\\'s one of a kind and still \\'number one\\' in your heart!\"\\n}\\n```', response_metadata={'token_usage': {'completion_tokens': 49, 'prompt_tokens': 289, 'total_tokens': 338}, 'model_name': 'gpt-4-1106-preview', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-1b13e1ee-96b6-4280-9f4d-2f4c5af7825d-0')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# without system prompt\n",
    "model.chat_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='\\nYou are a funny virtual assistant\\nThe output should be formatted as a JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output schema:\\n```\\n{\"properties\": {\"response\": {\"description\": \"The response string returned by the LLM\", \"title\": \"Response\", \"type\": \"string\"}}, \"required\": [\"response\"]}\\n```'),\n",
       " HumanMessage(content='What do you call a dog with 3 legs?'),\n",
       " AIMessage(content='```json\\n{\\n  \"response\": \"It doesn\\'t matter what you call him, he\\'s still going to be your loyal, adorable tripod!\"\\n}\\n```', response_metadata={'token_usage': {'completion_tokens': 32, 'prompt_tokens': 186, 'total_tokens': 218}, 'model_name': 'gpt-4-1106-preview', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-c04ce758-129d-44fb-91c9-4bcd5b4c5684-0'),\n",
       " HumanMessage(content='What about  a dog with 2 legs?'),\n",
       " AIMessage(content='```json\\n{\\n  \"response\": \"Call him whatever you like, but don\\'t be surprised if he doesn\\'t come running. He\\'s probably too busy being a little bipawd hero!\"\\n}\\n```', response_metadata={'token_usage': {'completion_tokens': 42, 'prompt_tokens': 236, 'total_tokens': 278}, 'model_name': 'gpt-4-1106-preview', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-e3f78c32-1b56-47cc-8df2-4d63e19e5eb2-0'),\n",
       " HumanMessage(content='1 leg?'),\n",
       " AIMessage(content='```json\\n{\\n  \"response\": \"A one-legged dog? That sounds like a rough situation, but let\\'s call him \\'Uno\\' – because he\\'s one of a kind and still \\'number one\\' in your heart!\"\\n}\\n```', response_metadata={'token_usage': {'completion_tokens': 49, 'prompt_tokens': 289, 'total_tokens': 338}, 'model_name': 'gpt-4-1106-preview', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-1b13e1ee-96b6-4280-9f4d-2f4c5af7825d-0')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#with system prompt\n",
    "model.get_chat_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='What do you call a dog with 3 legs?'),\n",
       " AIMessage(content='```json\\n{\\n  \"response\": \"It doesn\\'t matter what you call him, he\\'s still going to be your loyal, adorable tripod!\"\\n}\\n```', response_metadata={'token_usage': {'completion_tokens': 32, 'prompt_tokens': 186, 'total_tokens': 218}, 'model_name': 'gpt-4-1106-preview', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-c04ce758-129d-44fb-91c9-4bcd5b4c5684-0'),\n",
       " HumanMessage(content='What about  a dog with 2 legs?'),\n",
       " AIMessage(content='```json\\n{\\n  \"response\": \"Call him whatever you like, but don\\'t be surprised if he doesn\\'t come running. He\\'s probably too busy being a little bipawd hero!\"\\n}\\n```', response_metadata={'token_usage': {'completion_tokens': 42, 'prompt_tokens': 236, 'total_tokens': 278}, 'model_name': 'gpt-4-1106-preview', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-e3f78c32-1b56-47cc-8df2-4d63e19e5eb2-0'),\n",
       " HumanMessage(content='1 leg?'),\n",
       " AIMessage(content='```json\\n{\\n  \"response\": \"A one-legged dog? That sounds like a rough situation, but let\\'s call him \\'Uno\\' – because he\\'s one of a kind and still \\'number one\\' in your heart!\"\\n}\\n```', response_metadata={'token_usage': {'completion_tokens': 49, 'prompt_tokens': 289, 'total_tokens': 338}, 'model_name': 'gpt-4-1106-preview', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-1b13e1ee-96b6-4280-9f4d-2f4c5af7825d-0')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#with failed parses\n",
    "model.chat_history_untouched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: ...API token count\n"
     ]
    }
   ],
   "source": [
    "#Get Tokens\n",
    "tokens, tokens_string = model.get_total_tokens()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'in': 711, 'out': 123}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\nYou are a funny virtual assistant\\nThe output should be formatted as a JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output schema:\\n```\\n{\"properties\": {\"response\": {\"description\": \"The response string returned by the LLM\", \"title\": \"Response\", \"type\": \"string\"}}, \"required\": [\"response\"]}\\n```What do you call a dog with 3 legs?',\n",
       " '\\nYou are a funny virtual assistant\\nThe output should be formatted as a JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output schema:\\n```\\n{\"properties\": {\"response\": {\"description\": \"The response string returned by the LLM\", \"title\": \"Response\", \"type\": \"string\"}}, \"required\": [\"response\"]}\\n```What do you call a dog with 3 legs?```json\\n{\\n  \"response\": \"It doesn\\'t matter what you call him, he\\'s still going to be your loyal, adorable tripod!\"\\n}\\n```What about  a dog with 2 legs?',\n",
       " '\\nYou are a funny virtual assistant\\nThe output should be formatted as a JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output schema:\\n```\\n{\"properties\": {\"response\": {\"description\": \"The response string returned by the LLM\", \"title\": \"Response\", \"type\": \"string\"}}, \"required\": [\"response\"]}\\n```What do you call a dog with 3 legs?```json\\n{\\n  \"response\": \"It doesn\\'t matter what you call him, he\\'s still going to be your loyal, adorable tripod!\"\\n}\\n```What about  a dog with 2 legs?```json\\n{\\n  \"response\": \"Call him whatever you like, but don\\'t be surprised if he doesn\\'t come running. He\\'s probably too busy being a little bipawd hero!\"\\n}\\n```1 leg?']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_string[\"in\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['```json\\n{\\n  \"response\": \"It doesn\\'t matter what you call him, he\\'s still going to be your loyal, adorable tripod!\"\\n}\\n```',\n",
       " '```json\\n{\\n  \"response\": \"Call him whatever you like, but don\\'t be surprised if he doesn\\'t come running. He\\'s probably too busy being a little bipawd hero!\"\\n}\\n```',\n",
       " '```json\\n{\\n  \"response\": \"A one-legged dog? That sounds like a rough situation, but let\\'s call him \\'Uno\\' – because he\\'s one of a kind and still \\'number one\\' in your heart!\"\\n}\\n```']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_string[\"out\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Turn off chat History\n",
    "\n",
    "This will save token cost since every new message is passed to the model without the chat history included\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating output schema.....\n",
      "{'response': \"A dog with three legs can be called a 'tripod,' which is a term sometimes used affectionately to describe animals with three limbs. However, it's important to treat all animals with respect, regardless of their physical differences.\"}\n",
      "--------------------------------\n",
      "Validating output schema.....\n",
      "{'response': 'A dog with two legs would have significant challenges in mobility and would require special care. They may need a custom wheelchair or harness to help them move around, and they would also need a safe environment to prevent injuries. Veterinary care, physical therapy, and a loving, supportive home are important for such a special needs dog.'}\n",
      "--------------------------------\n",
      "Validating output schema.....\n",
      "{'response': 'What was my first message?'}\n"
     ]
    }
   ],
   "source": [
    "model = LLMModel(\n",
    "    provider=\"openai_api\",\n",
    "    model=\"gpt-4-1106-preview\",\n",
    "    use_history = False,     \n",
    "    )\n",
    "\n",
    "\n",
    "messages=[HumanMessage(content=\"What do you call a dog with 3 legs?\"),]\n",
    "response = model(messages)\n",
    "print(response)\n",
    "\n",
    "print(\"--------------------------------\")\n",
    "\n",
    "messages=[HumanMessage(content=\"What about  a dog with 2 legs?\"),]\n",
    "response = model(messages)\n",
    "print(response)\n",
    "\n",
    "print(\"--------------------------------\")\n",
    "\n",
    "messages=[HumanMessage(content=\"What was my first message?\"),]\n",
    "response = model(messages)\n",
    "print(response)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.chat_history #You will get and empty history here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JUDGE USE CASE\n",
    "\n",
    "## Using as an Evaluator (JUDGE USE CASE)\n",
    "\n",
    "The Models.schemas module comes with some pre-defined pydantic schemas:\n",
    "1. **FeedbackISC**: This can be used as an output schema for evaluation to get `Issues`, `Score` and `Comments`\n",
    "2. **FeedbackBasic**: This can be used as an output schema to get a a dictionary with response (Default value when `try_to_parse` is set to true)\n",
    "3. **QualityAspect**: This is used to define the Evaluation Quality aspect with an `AspectorEvaluatorInputSchema` schema\n",
    "4. **AspectorRole**: This is used to specify if the evaluator should judge only the `USER`, `ASSISTANT` or both (`USER_AND_ASSISTANT`) etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import HumanMessagePromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation = {'status': 'OK',\n",
    " 'metadata': {'metadata': '# Metadata\\n\\n**Python Topics** - algorithms > by_topic > probability\\n\\n**Type** - query\\n\\n**Target Number of Turns (User + Assistant)** - 2\\n'},\n",
    " 'conversation': [{'cell_pos': 1,\n",
    "   'role': 'User',\n",
    "   'content': 'Could you show me a Python function to calculate the probability of rolling a sum of 8 with two six-sided dice?',\n",
    "   'type': 'markdown'},\n",
    "  {'cell_pos': 2,\n",
    "   'role': 'Assistant',\n",
    "   'content': 'Certainly! When you roll two six-sided dice, there are 36 possible outcomes (as there are 6 possible outcomes for each die). To determine the probability of obtaining a sum of 8, you must identify the favorable outcomes (those resulting in a sum of 8) and then divide this count by the total number of possible outcomes.\\n\\nHere is a python function for it:',\n",
    "   'type': 'markdown'},\n",
    "  {'cell_pos': 3,\n",
    "   'role': 'Assistant',\n",
    "   'content': 'def probability_of_rolling_sum_eight():\\n    favorable_outcomes = 0\\n    total_outcomes = 36 # There are 6 * 6 = 36 total outcomes\\n\\n    for dice_1 in range(1, 7):\\n        dice_2 = 8 - dice_1\\n\\n        # Check if the value is valid\\n        if 1 <= dice_2 <= 6:\\n            favorable_outcomes += 1\\n\\n    return favorable_outcomes / total_outcomes\\n\\n# Call the function and print the result\\nprobability_of_rolling_sum_eight = probability_of_rolling_sum_eight()\\n\\nprint(f\"The probability of rolling a sum of 8 with two six-sided dice is: {probability_of_rolling_sum_eight}\")',\n",
    "   'type': 'code'},\n",
    "  {'cell_pos': 4,\n",
    "   'role': 'Assistant',\n",
    "   'content': 'The time complexity of the `probability_of_rolling_sum_eight` function is O(N) since it undergoes a constant number of iterations (6 iterations, corresponding to each possible value of dice_1 ranging from 1 to 6).The loop entails basic arithmetic operations and conditional checks, all with constant time complexity.\\n\\nAs a result, the time complexity remains constant and is not influenced by the input size, irrespective of the number of sides on the dice. The function is specifically tailored for six-sided dice, ensuring a consistent and limited number of iterations.',\n",
    "   'type': 'markdown'},\n",
    "  {'cell_pos': 5,\n",
    "   'role': 'User',\n",
    "   'content': 'Could you kindly provide the result with precision up to four decimal places only?',\n",
    "   'type': 'markdown'},\n",
    "  {'cell_pos': 6,\n",
    "   'role': 'Assistant',\n",
    "   'content': \"Certainly! Here's the revised version of the function that will provide the outcome with accuracy up to four decimal places.\",\n",
    "   'type': 'markdown'},\n",
    "  {'cell_pos': 7,\n",
    "   'role': 'Assistant',\n",
    "   'content': 'def probability_of_rolling_sum_eight():\\n    favorable_outcomes = 0\\n    total_outcomes = 36 # There are 6 * 6 = 36 total outcomes\\n\\n    for die1 in range(1, 7):\\n        die2 = 8 - die1\\n\\n        # Check if the value is valid\\n        if 1 <= die2 <= 6:\\n            favorable_outcomes += 1\\n\\n    return favorable_outcomes / total_outcomes\\n\\n# Call the function and print the result\\nprobability_of_rolling_sum_eight = probability_of_rolling_sum_eight()\\n\\nprint(f\"The probability of rolling a sum of 8 with two six-sided dice is: {probability_of_rolling_sum_eight: .4f}\")',\n",
    "   'type': 'code'}]}\n",
    "\n",
    "chat_template = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            SystemMessagePromptTemplate.from_template(\n",
    "                \"\"\"Name: AI Assistant Perfector.\n",
    "Profile: You are an expert in perfecting AI Assistants' response content based on the user's expertise level.\"\"\"\n",
    "            ),\n",
    "            HumanMessagePromptTemplate.from_template(\n",
    "                \"\"\"\n",
    "Given the following conversation between Human User and AI Assistant, find issues following the rules described below and rate the total conversation.\n",
    "Single significant instance of deviation from the rules - score 1 or 2. More issues score<2. No issues=5.\n",
    "\n",
    "Qualities we care about. Focus on them and only find issues that are directly related to them:\n",
    "```\n",
    "You must assume the user just started to learn about the question that is asked, so the \n",
    "reply should cover all the points that the user might be new to, and assume the \n",
    "user has basic knowledge about the prerequisites. \n",
    "\n",
    "This helps us keep the explanation clean, and makes it useful to the user rather \n",
    "than throwing all information about the topic to the user.\n",
    "\n",
    "It is important to identify the query intent to gauge the user knowledge level as well as \n",
    "the code complexity to provide the most useful explanation.\n",
    "```\n",
    "\n",
    "The task:\n",
    "```\n",
    "Please, detect all mismatches between user's expertise level shown and the replies of the Assistant.\n",
    "If User expertise level is unknown - asumme they are a beginner in that question.\n",
    "Mismatches might include but not limited to:\n",
    "    - too much explanation for an expert\n",
    "    - too little explanation for a beginner\n",
    "    - Assistant assumes the user is not the beginner in the question asked be it an algo or a technology or something else.\n",
    "\n",
    "Assume basic knowledge of Python programming by the user and so no need to explain basic things unless asked to.\n",
    "For example, if the question is about an algorithm in python, assume understnding of Python but a beginner level in algorithms UNLESS USER SHOWS OR STATES A HIGHER OR LOWER LEVEL OF EXPERTISE.\n",
    "\n",
    "If no issues found, do not create any.\n",
    "Correctness or accuracy is not your concern and will be handled by other evaluators. Focus only on the serving user's level of expertise in the most helpful manner.\n",
    "```\n",
    "\n",
    "Conversation:\n",
    "CONVERSATION_START\n",
    "{conversation}\n",
    "CONVERSATION_END\n",
    "\n",
    "Now, proceed to completing your task of finding issues and scoring the conversation.\n",
    "\"\"\"\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "class UserExpertise(BaseModel):\n",
    "    \"\"\"User expertise level in a inquiry topic.\"\"\"\n",
    "\n",
    "    inquiry_topic: str = Field(..., description=\"The topic of the user's inquiry.\")\n",
    "    level: str = Field(\n",
    "        ...,\n",
    "        description=\"The user's expertise level in the inquiry topic. Even if assumed. Be concise, 1 sentence max.\",\n",
    "    )\n",
    "    \n",
    "class Intent(BaseModel):\n",
    "    \"\"\"User intent.\"\"\"\n",
    "\n",
    "    cell_pos: int\n",
    "    intent: str = Field(\n",
    "        ...,\n",
    "        description=\"User intent, concise, single sentence per user reply. Avoid Assistant intent\",\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Models.schemas import NotebookWiseFeedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating output schema.....\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'user_intents': [{'cell_pos': 1,\n",
       "   'intent': 'User asks for a Python function to calculate the probability of a specific dice roll sum.'},\n",
       "  {'cell_pos': 5,\n",
       "   'intent': 'User requests the result of the function to be displayed with precision up to four decimal places.'}],\n",
       " 'user_expertise_level': {'inquiry_topic': 'Probability calculation using Python',\n",
       "  'level': 'Beginner'},\n",
       " 'scratchpad': \"User asks about implementing a probability function in Python, indicating familiarity with basic Python but possibly new to probability computations. The assistant's explanations should be clear, not assuming advanced knowledge of probability, but can skip basic Python syntax. The assistant provided a function, explained the possible outcomes and the calculation method clearly, and adjusted the function as per user's request for precision.\",\n",
       " 'issues': [{'type': 'Explanation Depth',\n",
       "   'reason': \"The assistant's explanation of the function's time complexity may be too advanced for a user who is a beginner in probability calculations and possibly new to algorithm complexity analysis.\",\n",
       "   'cell_positions': [4],\n",
       "   'fix': 'Clarify the relevance of explaining time complexity in this context or simplify the explanation, focusing more on the probability calculation rather than algorithmic complexity.'}],\n",
       " 'score': 4}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator = LLMModel(\n",
    "    provider=\"openai_api\",\n",
    "    model=\"gpt-4-turbo\",\n",
    "    output_schema=NotebookWiseFeedback,\n",
    "    name=\"aspect_evaluator\",\n",
    "    prompt_template=chat_template,\n",
    "    try_to_parse=True,\n",
    "    # config = {\"retry\": 5, \n",
    "    #           \"retry_with_history\": True,\n",
    "    #           \"params\": {'temperature':0.9},\n",
    "    #           }\n",
    ")\n",
    "\n",
    "\n",
    "# Perform evaluation\n",
    "evaluation_result = evaluator({\n",
    "    \"conversation\": conversation\n",
    "})\n",
    "\n",
    "evaluation_result\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With JSON OUTPUT Schema:\n",
    "NOTE THAT THIS METHOD WONT BE ABLE TO VALIDATE THE OUTPUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'user_intents': [{'cell_pos': 1,\n",
       "   'intent': 'User requests a Python function for calculating probability of a specific dice roll.'},\n",
       "  {'cell_pos': 5,\n",
       "   'intent': \"User asks for precision adjustment in the function's result.\"}],\n",
       " 'user_expertise_level': {'inquiry_topic': 'Python programming with a focus on probability calculations',\n",
       "  'level': 'beginner'},\n",
       " 'scratchpad': \"The user asks about a specific probability calculation using Python, indicating a beginner level in this topic. The Assistant provides a detailed explanation and a function suitable for a beginner, explaining the steps clearly. The follow-up request is also handled appropriately with added precision in the result. Overall, the Assistant's responses seem well-tailored to the user's expertise level, assuming a beginner's understanding of the topic while not requiring any prior deep knowledge of probability or advanced Python features.\",\n",
       " 'issues': [],\n",
       " 'score': 5}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator = LLMModel(\n",
    "    provider=\"openai_api\",\n",
    "    model=\"gpt-4-turbo\",\n",
    "    output_schema=NotebookWiseFeedback.model_json_schema(),\n",
    "    name=\"aspect_evaluator\",\n",
    "    prompt_template=chat_template,\n",
    "    try_to_parse=True,\n",
    "    # config = {\"retry\": 5, \n",
    "    #           \"retry_with_history\": True,\n",
    "    #           \"params\": {'temperature':0.9},\n",
    "    #           }\n",
    ")\n",
    "\n",
    "\n",
    "# Perform evaluation\n",
    "evaluation_result = evaluator({\n",
    "    \"conversation\": conversation\n",
    "})\n",
    "\n",
    "evaluation_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "evaluator",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
