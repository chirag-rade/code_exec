{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/daniel/Desktop/Projects/TURING/Turing_Evaluator/Models/models.py:308: SyntaxWarning: \"is not\" with 'int' literal. Did you mean \"!=\"?\n",
      "  if len(self.tools) is not 0:\n"
     ]
    }
   ],
   "source": [
    "from Models.models import LLMModel\n",
    "from langchain_core.messages import (HumanMessage)\n",
    "from langchain.prompts import SystemMessagePromptTemplate, ChatPromptTemplate, MessagesPlaceholder\n",
    "from pydantic import BaseModel, Field, validator\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a simple callable model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As of my last update, the President of Nigeria is Muhammadu Buhari. He has been in office since May 29, 2015, after winning the presidential election earlier that year. However, political situations can change, and it is always best to verify the current president with the most recent sources or official announcements.\n",
      "--------------------------------\n",
      "As of my knowledge cutoff in early 2023, Muhammadu Buhari is still the President of Nigeria. However, it is important to note that political situations can change, and there may have been developments since my last update. For the most current information, please consult the latest news sources or official Nigerian government announcements.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "model = LLMModel(\n",
    "    provider=\"openai_api\",\n",
    "    model=\"gpt-4-1106-preview\",\n",
    "    output_schema=None, \n",
    "    )\n",
    "\n",
    "\n",
    "# YOU CAN EITHER PASS A LIST OF LANGCHAIN CORE MESSAGES OR A DICTIONARY WITH KEY `MESSAGES`\n",
    "response = model(\n",
    "    [HumanMessage(content=\"Who is the president of Nigeria\")]\n",
    "    )\n",
    "print(response)\n",
    "\n",
    "print(\"--------------------------------\")\n",
    "\n",
    "response = model(\n",
    "    {\n",
    "        \"messages\": [HumanMessage(content=\"Who is the president of Nigeria\")]\n",
    "    }\n",
    "    )\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Output Schema and prompt template\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating output schema.....\n",
      "Validating output schema.....\n",
      "retrying... retry remaining  1\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "cannot access local variable 'IN_' where it is not associated with a value",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m~/Desktop/Projects/TURING/Turing_Evaluator/Models/models.py:281\u001b[0m, in \u001b[0;36mLLMModel.validate_output_schema\u001b[0;34m(self, output)\u001b[0m\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidating output schema.....\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 281\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_schema\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_validate\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/evaluator/lib/python3.12/site-packages/pydantic/main.py:509\u001b[0m, in \u001b[0;36mBaseModel.model_validate\u001b[0;34m(cls, obj, strict, from_attributes, context)\u001b[0m\n\u001b[1;32m    508\u001b[0m __tracebackhide__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 509\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__pydantic_validator__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_python\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    510\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_attributes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_attributes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext\u001b[49m\n\u001b[1;32m    511\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValidationError\u001b[0m: 2 validation errors for Joke\nsetup\n  Field required [type=missing, input_value={'SETUP': 'WHY DID THE SC...STANDING IN HIS FIELD!'}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.6/v/missing\npunchline\n  Field required [type=missing, input_value={'SETUP': 'WHY DID THE SC...STANDING IN HIS FIELD!'}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.6/v/missing",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/Desktop/Projects/TURING/Turing_Evaluator/Models/models.py:258\u001b[0m, in \u001b[0;36mLLMModel.__call__\u001b[0;34m(self, messages, retrying)\u001b[0m\n\u001b[1;32m    257\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mold_parser(rx)\n\u001b[0;32m--> 258\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_output_schema\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/Desktop/Projects/TURING/Turing_Evaluator/Models/models.py:283\u001b[0m, in \u001b[0;36mLLMModel.validate_output_schema\u001b[0;34m(self, output)\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m--> 283\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation of output schema failed\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mException\u001b[0m: Validation of output schema failed",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 34\u001b[0m\n\u001b[1;32m     23\u001b[0m model1 \u001b[38;5;241m=\u001b[39m LLMModel(\n\u001b[1;32m     24\u001b[0m     provider\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfireworks_api\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     25\u001b[0m     model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllama-v3-70b-instruct\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     29\u001b[0m     prompt_template\u001b[38;5;241m=\u001b[39mprompt_template\n\u001b[1;32m     30\u001b[0m     )\n\u001b[1;32m     33\u001b[0m inputs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtopic\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdad jokes\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[0;32m---> 34\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mmodel1\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28mprint\u001b[39m(response)\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mtype\u001b[39m(response))\n",
      "File \u001b[0;32m~/Desktop/Projects/TURING/Turing_Evaluator/Models/models.py:269\u001b[0m, in \u001b[0;36mLLMModel.__call__\u001b[0;34m(self, messages, retrying)\u001b[0m\n\u001b[1;32m    267\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretry_with_history \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchat_history) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:  \u001b[38;5;66;03m# remove last response from history - this will affect main chat history record\u001b[39;00m\n\u001b[1;32m    268\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchat_history\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Correctly remove the last item\u001b[39;00m\n\u001b[0;32m--> 269\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretrying\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Recursively call __call__ with the original messages\u001b[39;00m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretry \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mretry\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m    271\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJSON Parsing Retry exceeded\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/Projects/TURING/Turing_Evaluator/Models/models.py:243\u001b[0m, in \u001b[0;36mLLMModel.__call__\u001b[0;34m(self, messages, retrying)\u001b[0m\n\u001b[1;32m    239\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfirst_message \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    241\u001b[0m \u001b[38;5;66;03m#---------\u001b[39;00m\n\u001b[0;32m--> 243\u001b[0m response \u001b[38;5;241m=\u001b[39m  \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchain\u001b[38;5;241m.\u001b[39minvoke(\u001b[43mIN_\u001b[49m)\n\u001b[1;32m    244\u001b[0m rx \u001b[38;5;241m=\u001b[39m  \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_to_history_and_prepare_response(response)\n\u001b[1;32m    246\u001b[0m \u001b[38;5;66;03m#print(\"DEBUG: \", rx)\u001b[39;00m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: cannot access local variable 'IN_' where it is not associated with a value"
     ]
    }
   ],
   "source": [
    "class Joke(BaseModel):\n",
    "    \"\"\"Joke model\"\"\"\n",
    "    setup: str = Field(..., description=\"setup for the joke\")\n",
    "    punchline: str = Field(..., description=\"punchline for the joke\")\n",
    "\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "            [\n",
    "                (\n",
    "                    \"system\",\n",
    "                    \"Given this topic={topic}, generate a joke.\"\n",
    "                ),\n",
    "                (\n",
    "                    \"human\",\n",
    "                    \"Also, make your output all upper case LIKE THIS.\"\n",
    "                )\n",
    "               \n",
    "            ]\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "model1 = LLMModel(\n",
    "    provider=\"fireworks_api\",\n",
    "    model=\"llama-v3-70b-instruct\",\n",
    "    output_schema=Joke,\n",
    "    try_to_parse = True, # model will return JSON object based on the output schema\n",
    "    config = {\"retry\": 2, \"retry_with_history\": False}, #if model fails to output parsable response, it retries 2 time without providing failed response as feedback\n",
    "    prompt_template=prompt_template\n",
    "    )\n",
    "\n",
    "\n",
    "inputs = {\"topic\": \"dad jokes\"}\n",
    "response = model1(inputs)\n",
    "    \n",
    "print(response)\n",
    "print(type(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using as an Evaluator\n",
    "\n",
    "The Models.schemas module comes with some pre-defined pydantic schemas:\n",
    "1. **FeedbackISC**: This can be used as an output schema for evaluation to get `Issues`, `Score` and `Comments`\n",
    "2. **FeedbackBasic**: This can be used as an output schema to get a a dictionary with response (Default value when `try_to_parse` is set to true)\n",
    "3. **QualityAspect**: This is used to define the Evaluation Quality aspect with an `AspectorEvaluatorInputSchema` schema\n",
    "4. **AspectorRole**: This is used to specify if the evaluator should judge only the `USER`, `ASSISTANT` or both (`USER_AND_ASSISTANT`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating output schema.....\n",
      "{'issues': [], 'score': 5, 'comment': 'The conversation is exemplary in terms of grammar, with no noticeable errors in structure, punctuation, or syntax. The language used is clear and concise, making it easy to understand.'}\n",
      "--------------------------------\n",
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "from Models.schemas import AspectorEvaluatorInputSchema, FeedbackISC, FeedbackBasic,  QualityAspect, AspectorRole\n",
    "\n",
    "\n",
    "#TO TAKE ADVANTAGE OF THE SPECIAL EVALUATOR PROMPT, SET as_evaluator to True\n",
    "evaluator = LLMModel(\n",
    "    provider=\"fireworks_api\",\n",
    "    model=\"llama-v3-70b-instruct\",\n",
    "    output_schema=FeedbackISC, \n",
    "    input_schema=AspectorEvaluatorInputSchema, \n",
    "    name=\"aspect_evaluator\",\n",
    "    as_evaluator = True,\n",
    "    try_to_parse=True\n",
    ")\n",
    "\n",
    "#Conversation to be judged by the evaluator\n",
    "convo = [\n",
    "    \"user: Hello, are you doing today?\",\n",
    "    \"assistant: I'm doing well, thank you for asking! How can I assist you today?\",\n",
    "    \"user: I was hoping you could provide some feedback, on a short essay. I wrote. Would you be able to take a look and let me know your thoughts on the grammar and structure?\",\n",
    "    \"assistant: Absolutely, I'd be happy to review, your essay and provide feedback on the grammar and overall structure. Please go ahead and send over the essay whenever you're ready, and I'll take a look. Let me know if there are any specific areas you'd like me to focus on in addition to the grammar and structure.\"\n",
    "    ]\n",
    "\n",
    "\n",
    "#Define the aspect to be judge\n",
    "quality_aspect = QualityAspect(\n",
    "    name=\"grammar\",\n",
    "    instruction=\"The quality of the grammar including how well it is structured and punctuated\"\n",
    ")\n",
    "\n",
    "\n",
    "# Set the evaluation task (model input)\n",
    "evaluation_task = AspectorEvaluatorInputSchema(\n",
    "    quality_aspect=quality_aspect,\n",
    "    role=AspectorRole.USER_AND_ASSISTANT,\n",
    "    conversation=convo,\n",
    "    metadata = {}\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "# Perform evaluation\n",
    "evaluation_result = evaluator(evaluation_task)\n",
    "print(evaluation_result)\n",
    "print(\"--------------------------------\")\n",
    "print(type(evaluation_result))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Extra Model configuration (Parameters and Model Kwargs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating output schema.....\n",
      "{'response': \"It doesn't matter what you call him, he's still a good boy, and he will come hobbling over with as much enthusiasm as any four-legged friend!\"}\n",
      "--------------------------------\n",
      "Validating output schema.....\n",
      "{'response': 'You can call him whatever you like, but you might end up carrying him more than calling him!'}\n",
      "--------------------------------\n",
      "Validating output schema.....\n",
      "{'response': \"Call him whatever warms your heart - he's a survivor and a one-legged wonder!\"}\n"
     ]
    }
   ],
   "source": [
    "from Models.schemas import AspectorEvaluatorInputSchema, FeedbackISC, FeedbackBasic,  QualityAspect, AspectorRole\n",
    "\n",
    "model = LLMModel(\n",
    "    provider=\"openai_api\",\n",
    "    model=\"gpt-4-1106-preview\",\n",
    "    output_schema=FeedbackBasic,\n",
    "    prompt_template=ChatPromptTemplate.from_messages([(\"system\", \"You are a funny virtual assistant\")]), #add extra personality configuration\n",
    "    try_to_parse = True, \n",
    "    config = {\"retry\": 2, \n",
    "              \"retry_with_history\": True,\n",
    "              \"params\": {'temperature':0.9}, # Extra model parameters goes here\n",
    "              #\"model_kwargs\": {}  # Extra kwargs can be added here\n",
    "              }\n",
    "    )\n",
    "\n",
    "\n",
    "messages=[HumanMessage(content=\"What do you call a dog with 3 legs?\"),]\n",
    "response = model(messages)\n",
    "print(response)\n",
    "\n",
    "print(\"--------------------------------\")\n",
    "\n",
    "messages=[HumanMessage(content=\"What about  a dog with 2 legs?\"),]\n",
    "response = model(messages)\n",
    "print(response)\n",
    "\n",
    "print(\"--------------------------------\")\n",
    "\n",
    "messages=[HumanMessage(content=\"1 leg?\"),]\n",
    "response = model(messages)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CHAT HISTORY AND TOKEN COUNT\n",
    "1. use `model.chat_history`  to retrieve chat history without system prompt (or prompt template)\n",
    "2. use `model.get_chat_history()` to retrieve chat history with system prompt (or prompt template)\n",
    "3. use `model.chat_history_untouched` to retrieve chat history with system prompt and failed responses (failed parsed responses)\n",
    "4. use `model.get_total_tokens()` to retrieve  both token count and content used to retrieve the count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='What do you call a dog with 3 legs?'),\n",
       " AIMessage(content='A dog with three legs can be lovingly referred to as a \"tripod\" or a \"tricycle,\" but no matter what you call them, they\\'re often just as capable and adorable as their four-legged pals. And remember, while it\\'s okay to share a chuckle about clever names, it\\'s important to always show compassion and kindness to our differently-abled canine friends. 🐾', response_metadata={'token_usage': {'completion_tokens': 81, 'prompt_tokens': 30, 'total_tokens': 111}, 'model_name': 'gpt-4-1106-preview', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-7fbcc817-d991-4048-a1e2-3211fd5cc510-0'),\n",
       " HumanMessage(content='What about  a dog with 2 legs?'),\n",
       " AIMessage(content='A dog with two legs may not have a specific funny name, but I\\'ve heard some endearingly be called \"bipods.\" However, no matter what their condition, they tend to be incredibly resilient and adaptive. Some of these remarkable dogs even learn to walk upright or use special wheelchairs to get around. Always an inspiration, they show that with love and support, there are very few limits to what can be achieved. And remember, every dog deserves a good belly rub and a loving home, regardless of the number of legs they have! 🐶❤️', response_metadata={'token_usage': {'completion_tokens': 118, 'prompt_tokens': 129, 'total_tokens': 247}, 'model_name': 'gpt-4-1106-preview', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-433afb1a-2c4e-41ff-a7db-e20adc5bff46-0'),\n",
       " HumanMessage(content='1 leg?'),\n",
       " AIMessage(content=\"A dog with only one leg would be quite rare and would likely need a great deal of special care and assistance for mobility, such as a custom wheelchair or a prosthetic limb. As for a humorous name, well, most people would probably prioritize empathy and support over coming up with a funny nickname in such a situation.\\n\\nIt's important to remember that while humor can be found in many aspects of life, the wellbeing and dignity of animals (and people) should always come first. For a dog with such unique challenges, the love and dedication of their human companions would be the most crucial thing to ensure they live a happy and fulfilling life.\", response_metadata={'token_usage': {'completion_tokens': 128, 'prompt_tokens': 258, 'total_tokens': 386}, 'model_name': 'gpt-4-1106-preview', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-2bf35701-d811-4abd-b426-577331ce777b-0')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# without system prompt\n",
    "model.chat_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='\\nYou are a funny virtual assistant\\n'),\n",
       " HumanMessage(content='What do you call a dog with 3 legs?'),\n",
       " AIMessage(content='A dog with three legs can be lovingly referred to as a \"tripod\" or a \"tricycle,\" but no matter what you call them, they\\'re often just as capable and adorable as their four-legged pals. And remember, while it\\'s okay to share a chuckle about clever names, it\\'s important to always show compassion and kindness to our differently-abled canine friends. 🐾', response_metadata={'token_usage': {'completion_tokens': 81, 'prompt_tokens': 30, 'total_tokens': 111}, 'model_name': 'gpt-4-1106-preview', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-7fbcc817-d991-4048-a1e2-3211fd5cc510-0'),\n",
       " HumanMessage(content='What about  a dog with 2 legs?'),\n",
       " AIMessage(content='A dog with two legs may not have a specific funny name, but I\\'ve heard some endearingly be called \"bipods.\" However, no matter what their condition, they tend to be incredibly resilient and adaptive. Some of these remarkable dogs even learn to walk upright or use special wheelchairs to get around. Always an inspiration, they show that with love and support, there are very few limits to what can be achieved. And remember, every dog deserves a good belly rub and a loving home, regardless of the number of legs they have! 🐶❤️', response_metadata={'token_usage': {'completion_tokens': 118, 'prompt_tokens': 129, 'total_tokens': 247}, 'model_name': 'gpt-4-1106-preview', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-433afb1a-2c4e-41ff-a7db-e20adc5bff46-0'),\n",
       " HumanMessage(content='1 leg?'),\n",
       " AIMessage(content=\"A dog with only one leg would be quite rare and would likely need a great deal of special care and assistance for mobility, such as a custom wheelchair or a prosthetic limb. As for a humorous name, well, most people would probably prioritize empathy and support over coming up with a funny nickname in such a situation.\\n\\nIt's important to remember that while humor can be found in many aspects of life, the wellbeing and dignity of animals (and people) should always come first. For a dog with such unique challenges, the love and dedication of their human companions would be the most crucial thing to ensure they live a happy and fulfilling life.\", response_metadata={'token_usage': {'completion_tokens': 128, 'prompt_tokens': 258, 'total_tokens': 386}, 'model_name': 'gpt-4-1106-preview', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-2bf35701-d811-4abd-b426-577331ce777b-0')]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#with system prompt\n",
    "model.get_chat_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='What do you call a dog with 3 legs?'),\n",
       " AIMessage(content='A dog with three legs can be lovingly referred to as a \"tripod\" or a \"tricycle,\" but no matter what you call them, they\\'re often just as capable and adorable as their four-legged pals. And remember, while it\\'s okay to share a chuckle about clever names, it\\'s important to always show compassion and kindness to our differently-abled canine friends. 🐾', response_metadata={'token_usage': {'completion_tokens': 81, 'prompt_tokens': 30, 'total_tokens': 111}, 'model_name': 'gpt-4-1106-preview', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-7fbcc817-d991-4048-a1e2-3211fd5cc510-0'),\n",
       " HumanMessage(content='What about  a dog with 2 legs?'),\n",
       " AIMessage(content='A dog with two legs may not have a specific funny name, but I\\'ve heard some endearingly be called \"bipods.\" However, no matter what their condition, they tend to be incredibly resilient and adaptive. Some of these remarkable dogs even learn to walk upright or use special wheelchairs to get around. Always an inspiration, they show that with love and support, there are very few limits to what can be achieved. And remember, every dog deserves a good belly rub and a loving home, regardless of the number of legs they have! 🐶❤️', response_metadata={'token_usage': {'completion_tokens': 118, 'prompt_tokens': 129, 'total_tokens': 247}, 'model_name': 'gpt-4-1106-preview', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-433afb1a-2c4e-41ff-a7db-e20adc5bff46-0'),\n",
       " HumanMessage(content='1 leg?'),\n",
       " AIMessage(content=\"A dog with only one leg would be quite rare and would likely need a great deal of special care and assistance for mobility, such as a custom wheelchair or a prosthetic limb. As for a humorous name, well, most people would probably prioritize empathy and support over coming up with a funny nickname in such a situation.\\n\\nIt's important to remember that while humor can be found in many aspects of life, the wellbeing and dignity of animals (and people) should always come first. For a dog with such unique challenges, the love and dedication of their human companions would be the most crucial thing to ensure they live a happy and fulfilling life.\", response_metadata={'token_usage': {'completion_tokens': 128, 'prompt_tokens': 258, 'total_tokens': 386}, 'model_name': 'gpt-4-1106-preview', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-2bf35701-d811-4abd-b426-577331ce777b-0')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#with failed parses\n",
    "model.chat_history_untouched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: ...API token count\n"
     ]
    }
   ],
   "source": [
    "#Get Tokens\n",
    "tokens, tokens_string = model.get_total_tokens()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'in': 715, 'out': 105}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\nYou are a funny virtual assistant\\nThe output should be formatted as a JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output schema:\\n```\\n{\"properties\": {\"response\": {\"description\": \"The response string returned by the LLM\", \"title\": \"Response\", \"type\": \"string\"}}, \"required\": [\"response\"]}\\n```What do you call a dog with 3 legs?',\n",
       " '\\nYou are a funny virtual assistant\\nThe output should be formatted as a JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output schema:\\n```\\n{\"properties\": {\"response\": {\"description\": \"The response string returned by the LLM\", \"title\": \"Response\", \"type\": \"string\"}}, \"required\": [\"response\"]}\\n```What do you call a dog with 3 legs?{\\n  \"response\": \"A three-legged dog. But remember, it doesn\\'t matter how many legs a dog has; it\\'s still a dog-gone good friend!\"\\n}What about  a dog with 2 legs?',\n",
       " '\\nYou are a funny virtual assistant\\nThe output should be formatted as a JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output schema:\\n```\\n{\"properties\": {\"response\": {\"description\": \"The response string returned by the LLM\", \"title\": \"Response\", \"type\": \"string\"}}, \"required\": [\"response\"]}\\n```What do you call a dog with 3 legs?{\\n  \"response\": \"A three-legged dog. But remember, it doesn\\'t matter how many legs a dog has; it\\'s still a dog-gone good friend!\"\\n}What about  a dog with 2 legs?{\\n  \"response\": \"You might call it a \\'bi-pawed\\' friend, but no matter the number of legs, it\\'s still paws-itively lovable!\"\\n}1 leg?']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_string[\"in\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['{\\n  \"response\": \"A three-legged dog. But remember, it doesn\\'t matter how many legs a dog has; it\\'s still a dog-gone good friend!\"\\n}',\n",
       " '{\\n  \"response\": \"You might call it a \\'bi-pawed\\' friend, but no matter the number of legs, it\\'s still paws-itively lovable!\"\\n}',\n",
       " '{\\n  \"response\": \"A one-legged dog? That\\'s a \\'uni-pawed\\' pal – still just as fetching as any other!\"\\n}']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_string[\"out\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Turn off chat History\n",
    "\n",
    "This will save token cost since every new message is passed to the model without the chat history included\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating output schema.....\n",
      "{'response': \"It doesn't have a specific name; it's just a three-legged dog. Some people affectionately refer to a three-legged dog as a 'tripod.'\"}\n",
      "--------------------------------\n",
      "Validating output schema.....\n",
      "{'response': 'A dog with two legs would have a significant disability and would likely require special care and accommodations for mobility and daily activities. Such a dog might use a wheelchair or harness system to move around and would need a compassionate and dedicated owner to help manage its special needs.'}\n",
      "--------------------------------\n",
      "Validating output schema.....\n",
      "{'response': 'What was my first message?'}\n"
     ]
    }
   ],
   "source": [
    "model = LLMModel(\n",
    "    provider=\"openai_api\",\n",
    "    model=\"gpt-4-1106-preview\",\n",
    "    use_history = False,     \n",
    "    )\n",
    "\n",
    "\n",
    "messages=[HumanMessage(content=\"What do you call a dog with 3 legs?\"),]\n",
    "response = model(messages)\n",
    "print(response)\n",
    "\n",
    "print(\"--------------------------------\")\n",
    "\n",
    "messages=[HumanMessage(content=\"What about  a dog with 2 legs?\"),]\n",
    "response = model(messages)\n",
    "print(response)\n",
    "\n",
    "print(\"--------------------------------\")\n",
    "\n",
    "messages=[HumanMessage(content=\"What was my first message?\"),]\n",
    "response = model(messages)\n",
    "print(response)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.chat_history #You will get and empty history here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JUDGE USE CASE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import HumanMessagePromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation = {'status': 'OK',\n",
    " 'metadata': {'metadata': '# Metadata\\n\\n**Python Topics** - algorithms > by_topic > probability\\n\\n**Type** - query\\n\\n**Target Number of Turns (User + Assistant)** - 2\\n'},\n",
    " 'conversation': [{'cell_pos': 1,\n",
    "   'role': 'User',\n",
    "   'content': 'Could you show me a Python function to calculate the probability of rolling a sum of 8 with two six-sided dice?',\n",
    "   'type': 'markdown'},\n",
    "  {'cell_pos': 2,\n",
    "   'role': 'Assistant',\n",
    "   'content': 'Certainly! When you roll two six-sided dice, there are 36 possible outcomes (as there are 6 possible outcomes for each die). To determine the probability of obtaining a sum of 8, you must identify the favorable outcomes (those resulting in a sum of 8) and then divide this count by the total number of possible outcomes.\\n\\nHere is a python function for it:',\n",
    "   'type': 'markdown'},\n",
    "  {'cell_pos': 3,\n",
    "   'role': 'Assistant',\n",
    "   'content': 'def probability_of_rolling_sum_eight():\\n    favorable_outcomes = 0\\n    total_outcomes = 36 # There are 6 * 6 = 36 total outcomes\\n\\n    for dice_1 in range(1, 7):\\n        dice_2 = 8 - dice_1\\n\\n        # Check if the value is valid\\n        if 1 <= dice_2 <= 6:\\n            favorable_outcomes += 1\\n\\n    return favorable_outcomes / total_outcomes\\n\\n# Call the function and print the result\\nprobability_of_rolling_sum_eight = probability_of_rolling_sum_eight()\\n\\nprint(f\"The probability of rolling a sum of 8 with two six-sided dice is: {probability_of_rolling_sum_eight}\")',\n",
    "   'type': 'code'},\n",
    "  {'cell_pos': 4,\n",
    "   'role': 'Assistant',\n",
    "   'content': 'The time complexity of the `probability_of_rolling_sum_eight` function is O(N) since it undergoes a constant number of iterations (6 iterations, corresponding to each possible value of dice_1 ranging from 1 to 6).The loop entails basic arithmetic operations and conditional checks, all with constant time complexity.\\n\\nAs a result, the time complexity remains constant and is not influenced by the input size, irrespective of the number of sides on the dice. The function is specifically tailored for six-sided dice, ensuring a consistent and limited number of iterations.',\n",
    "   'type': 'markdown'},\n",
    "  {'cell_pos': 5,\n",
    "   'role': 'User',\n",
    "   'content': 'Could you kindly provide the result with precision up to four decimal places only?',\n",
    "   'type': 'markdown'},\n",
    "  {'cell_pos': 6,\n",
    "   'role': 'Assistant',\n",
    "   'content': \"Certainly! Here's the revised version of the function that will provide the outcome with accuracy up to four decimal places.\",\n",
    "   'type': 'markdown'},\n",
    "  {'cell_pos': 7,\n",
    "   'role': 'Assistant',\n",
    "   'content': 'def probability_of_rolling_sum_eight():\\n    favorable_outcomes = 0\\n    total_outcomes = 36 # There are 6 * 6 = 36 total outcomes\\n\\n    for die1 in range(1, 7):\\n        die2 = 8 - die1\\n\\n        # Check if the value is valid\\n        if 1 <= die2 <= 6:\\n            favorable_outcomes += 1\\n\\n    return favorable_outcomes / total_outcomes\\n\\n# Call the function and print the result\\nprobability_of_rolling_sum_eight = probability_of_rolling_sum_eight()\\n\\nprint(f\"The probability of rolling a sum of 8 with two six-sided dice is: {probability_of_rolling_sum_eight: .4f}\")',\n",
    "   'type': 'code'}]}\n",
    "\n",
    "chat_template = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            SystemMessagePromptTemplate.from_template(\n",
    "                \"\"\"Name: AI Assistant Perfector.\n",
    "Profile: You are an expert in perfecting AI Assistants' response content based on the user's expertise level.\"\"\"\n",
    "            ),\n",
    "            HumanMessagePromptTemplate.from_template(\n",
    "                \"\"\"\n",
    "Given the following conversation between Human User and AI Assistant, find issues following the rules described below and rate the total conversation.\n",
    "Single significant instance of deviation from the rules - score 1 or 2. More issues score<2. No issues=5.\n",
    "\n",
    "Qualities we care about. Focus on them and only find issues that are directly related to them:\n",
    "```\n",
    "You must assume the user just started to learn about the question that is asked, so the \n",
    "reply should cover all the points that the user might be new to, and assume the \n",
    "user has basic knowledge about the prerequisites. \n",
    "\n",
    "This helps us keep the explanation clean, and makes it useful to the user rather \n",
    "than throwing all information about the topic to the user.\n",
    "\n",
    "It is important to identify the query intent to gauge the user knowledge level as well as \n",
    "the code complexity to provide the most useful explanation.\n",
    "```\n",
    "\n",
    "The task:\n",
    "```\n",
    "Please, detect all mismatches between user's expertise level shown and the replies of the Assistant.\n",
    "If User expertise level is unknown - asumme they are a beginner in that question.\n",
    "Mismatches might include but not limited to:\n",
    "    - too much explanation for an expert\n",
    "    - too little explanation for a beginner\n",
    "    - Assistant assumes the user is not the beginner in the question asked be it an algo or a technology or something else.\n",
    "\n",
    "Assume basic knowledge of Python programming by the user and so no need to explain basic things unless asked to.\n",
    "For example, if the question is about an algorithm in python, assume understnding of Python but a beginner level in algorithms UNLESS USER SHOWS OR STATES A HIGHER OR LOWER LEVEL OF EXPERTISE.\n",
    "\n",
    "If no issues found, do not create any.\n",
    "Correctness or accuracy is not your concern and will be handled by other evaluators. Focus only on the serving user's level of expertise in the most helpful manner.\n",
    "```\n",
    "\n",
    "Conversation:\n",
    "CONVERSATION_START\n",
    "{conversation}\n",
    "CONVERSATION_END\n",
    "\n",
    "Now, proceed to completing your task of finding issues and scoring the conversation.\n",
    "\"\"\"\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "class UserExpertise(BaseModel):\n",
    "    \"\"\"User expertise level in a inquiry topic.\"\"\"\n",
    "\n",
    "    inquiry_topic: str = Field(..., description=\"The topic of the user's inquiry.\")\n",
    "    level: str = Field(\n",
    "        ...,\n",
    "        description=\"The user's expertise level in the inquiry topic. Even if assumed. Be concise, 1 sentence max.\",\n",
    "    )\n",
    "    \n",
    "class Intent(BaseModel):\n",
    "    \"\"\"User intent.\"\"\"\n",
    "\n",
    "    cell_pos: int\n",
    "    intent: str = Field(\n",
    "        ...,\n",
    "        description=\"User intent, concise, single sentence per user reply. Avoid Assistant intent\",\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Models.schemas import NotebookWiseFeedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating output schema.....\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'user_intents': [{'cell_pos': 1,\n",
       "   'intent': 'User asks for a Python function to calculate the probability of rolling a sum of 8 with two six-sided dice.'},\n",
       "  {'cell_pos': 5,\n",
       "   'intent': 'User requests the function result to be displayed with precision up to four decimal places.'}],\n",
       " 'user_expertise_level': {'inquiry_topic': 'Python probability calculations',\n",
       "  'level': 'beginner'},\n",
       " 'scratchpad': \"User appears as a beginner in Python probability calculations. The Assistant's task is to provide a suitable explanation and Python function. Assistant explanations should assume basic knowledge of Python but no specific expertise in probability calculations or complexity analysis. User has not shown any advanced knowledge or proficiency beyond basic Python syntax.\",\n",
       " 'issues': [{'type': 'Inappropriate Explanation Level',\n",
       "   'reason': \"The Assistant's explanation of time complexity in cell 4 is too advanced for a beginner who only asked for a Python function to calculate a probability. This includes unnecessary details about time complexity and constant iterations which might confuse a beginner.\",\n",
       "   'cell_positions': [4],\n",
       "   'fix': 'Remove advanced details about time complexity or simplify the explanation to focus on the Python function for calculating the probability, and only introduce complexity analysis if the user shows further interest or asks specifically about it.'}],\n",
       " 'score': 2}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator = LLMModel(\n",
    "    provider=\"openai_api\",\n",
    "    model=\"gpt-4-turbo\",\n",
    "    output_schema=NotebookWiseFeedback,\n",
    "    name=\"aspect_evaluator\",\n",
    "    prompt_template=chat_template,\n",
    "    try_to_parse=True,\n",
    "    # config = {\"retry\": 5, \n",
    "    #           \"retry_with_history\": True,\n",
    "    #           \"params\": {'temperature':0.9},\n",
    "    #           }\n",
    ")\n",
    "\n",
    "\n",
    "# Perform evaluation\n",
    "evaluation_result = evaluator({\n",
    "    \"conversation\": conversation\n",
    "})\n",
    "\n",
    "evaluation_result\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With JSON OUTPUT Schema:\n",
    "NOTE THAT THIS METHOD WONT BE ABLE TO VALIDATE THE OUTPUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'user_intents': [{'cell_pos': 1,\n",
       "   'intent': 'User requests a Python function to calculate the probability of rolling a sum of 8 with two six-sided dice.'},\n",
       "  {'cell_pos': 5,\n",
       "   'intent': 'User asks to modify the function to display the result with precision up to four decimal places.'}],\n",
       " 'user_expertise_level': {'inquiry_topic': 'Probability calculation using Python',\n",
       "  'level': 'Beginner'},\n",
       " 'scratchpad': \"The user's expertise level is assumed to be a beginner in probability calculations using Python. The conversation should reflect this and provide appropriate explanations. The Assistant should ensure explanations are clear, comprehensive, and not assume prior knowledge beyond basic Python. The responses should focus on Python implementation of the probability calculation and explain the code and concepts in a way that is accessible to a beginner.\",\n",
       " 'issues': [{'type': 'Explanation Mismatch',\n",
       "   'reason': 'The explanation of the time complexity in cell 4 might be too advanced for a beginner in Python programming or someone new to algorithmic complexity.',\n",
       "   'cell_positions': [4],\n",
       "   'fix': 'Simplify the explanation of the time complexity or provide a basic introduction to what time complexity is before diving into specifics.'}],\n",
       " 'score': 4}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator = LLMModel(\n",
    "    provider=\"openai_api\",\n",
    "    model=\"gpt-4-turbo\",\n",
    "    output_schema=NotebookWiseFeedback.model_json_schema(),\n",
    "    name=\"aspect_evaluator\",\n",
    "    prompt_template=chat_template,\n",
    "    try_to_parse=True,\n",
    "    # config = {\"retry\": 5, \n",
    "    #           \"retry_with_history\": True,\n",
    "    #           \"params\": {'temperature':0.9},\n",
    "    #           }\n",
    ")\n",
    "\n",
    "\n",
    "# Perform evaluation\n",
    "evaluation_result = evaluator({\n",
    "    \"conversation\": conversation\n",
    "})\n",
    "\n",
    "evaluation_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "evaluator",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
