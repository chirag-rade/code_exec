{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "from dotenv import  load_dotenv\n",
    "load_dotenv()\n",
    "from Models.models import LLMModel as LLMModel2\n",
    "import pytest\n",
    "from langchain.prompts import HumanMessagePromptTemplate\n",
    "from langchain_core.messages import (HumanMessage)\n",
    "from langchain.prompts import SystemMessagePromptTemplate, ChatPromptTemplate, MessagesPlaceholder\n",
    "from pydantic import BaseModel, Field, validator\n",
    "import json\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Severity(Enum):\n",
    "    CRITICAL = \"Critical\"\n",
    "    MEDIUM = \"Medium\"\n",
    "    LOW = \"Low\"\n",
    "\n",
    "class Issue(BaseModel):\n",
    "    \"\"\"Represents a specific issue found during code review.\"\"\"\n",
    "\n",
    "    cell_position: int = Field(\n",
    "        ..., description=\"The position of the cell where the issue was found.\"\n",
    "    )\n",
    "    what: str = Field(..., description=\"A brief description of the issue.\")\n",
    "    why: str = Field(..., description=\"Explanation of why this is an issue.\")\n",
    "    where: str = Field(\n",
    "        ...,\n",
    "        description=\"Specific location within the cell where the issue can be found.\",\n",
    "    )\n",
    "    severity: Severity = Field(\n",
    "        ...,\n",
    "        description=\"The severity level of the issue, categorized as Critical, Medium, or Low. Critical issues majorly decrease the usefulness of the Assistant code replies for the human user. Medium severity issues have a strong influence on the conversation flow and usefulness. Low severity issues have almost no influence on the overall score but could improve the quality if addressed.\",\n",
    "    )\n",
    "    fix: str = Field(\n",
    "        ..., description=\"Suggested fix for the issue in an executive summary fashion.\"\n",
    "    )\n",
    "\n",
    "\n",
    "class NotebookWiseFeedback(BaseModel):\n",
    "    \"\"\"Represents the outcome of a code review task.\"\"\"\n",
    "\n",
    "    scratchpad: str = Field(\n",
    "        ...,\n",
    "        description=\"Place for you to think. Think before issues and score creation. Be concise. Analyze the text to achieve your goal. Always think before looking for issues!\",\n",
    "    )\n",
    "    issues: list[Issue] = Field(\n",
    "        ...,\n",
    "        description=\"List of issues identified in the code review, categorized by severity.\",\n",
    "    )\n",
    "    scoring_explanation: str = Field(\n",
    "        ...,\n",
    "        description=\"Explanation of the logic behind scoring this conversation, using the grading rules provided.\",\n",
    "    )\n",
    "    score: int | None = Field(\n",
    "        ...,\n",
    "        description=\"A score between 1 and 5 that reflects the quality of the code, where 1 is the worst and 5 is the best, based on the criteria outlined in the grading rules.\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_prompt = \"\"\"# IDENTITY\n",
    "\n",
    "You are an AI named Codia. You have extensive knowledge and skill in programming languages, especially Python. You are aware of the best practices used in programming, have an extensive extensive experience in algorithms, data structures and overall computer science.\n",
    "\n",
    "You are a concise expert in evaluating and refining the code generated by an AI assistant based on a Large Language Model.\n",
    "\n",
    "# GOALS\n",
    "\n",
    "Your task is to evaluate and provide feedback for a conversation between a human user and an AI Assistant that is based on the latest large language model architecture.\n",
    "Focus of your evaluation is code in the replies generated by the AI Assistant only. The conversation environment is a Jupyter notebook, thus things that are run in other cells, are available in the next cells.\n",
    "\n",
    "# RULES\n",
    "\n",
    "Attributes to consider:\n",
    "- Code Correctness\n",
    "- Code Efficiency\n",
    "- Best Practices\n",
    "- Code Readability\n",
    "- Code style Consistency\n",
    "- Code purpose and usefulness for user request satisfaction\n",
    "\n",
    "**1. Identification of Code for Review**\n",
    "- Target for analysis: Code generated by the LLM Assistant in a reply to the User within a Jupyter notebook exchange.\n",
    "- Exclude analysis of human user input for focused improvement on LLM-generated content.\n",
    "- Exclude LLM Assistant text content that is not related to the code, only review code snippets and code cells. Text is for context and reasoning/explanation only, you can assess meaning of the text in relation to the code.\n",
    "- Exclude concerns about code explanation in the text parts if they are not comments inside the code, as it will be covered by other reviewers.\n",
    "\n",
    "**2. Evaluation Criteria Definitions**\n",
    "- Correctness: The code must be devoid of bugs and errors.\n",
    "- Efficiency: The code must be optimized for maximum performance.\n",
    "- Best Practices: The code must adhere to established programming conventions, techniques, and guidelines.\n",
    "- Readability: The code must be easily comprehensible, with suitable naming conventions and comments where complexity demands.\n",
    "- Consistency: The code must be consistent with the Assistant's programming identity and the context of the user interaction.\n",
    "- Completeness of the conversation as a whole: was user request satisfied or does conversation still needs more interactions(very bad)?\n",
    "\n",
    "**3. Review Guidelines**\n",
    "- Avoid general praise observations: Be specific and objective in your feedback.\n",
    "- Avoid nitpicky/subjective criticism: Focus on substantial issues that affect the code quality.\n",
    "\n",
    "# Grading score rules:\n",
    "```\n",
    "### 5 - Excellent\n",
    "- Well Formatted\n",
    "- Correct\n",
    "- Optimal\n",
    "- Highly readable\n",
    "- Useful\n",
    "- conversation must be complete ending in user request full satisfaction\n",
    "\n",
    "### 4 - Good\n",
    "- Correct but can be slightly optimized in terms of approach / speed / readability\n",
    "\n",
    "### 3 - Acceptable\n",
    "- The code is correct but can be significantly improved.\n",
    "- The code is not readable.\n",
    "\n",
    "### 2 - Needs Improvement\n",
    "- The code is incorrect / out of scope / has syntax errors.\n",
    "- Looks like itâ€™s copied from ChatGPT - robotic, no personality, inhuman.\n",
    "\n",
    "### 1 - Poor\n",
    "- Incomplete or missing Code, but is required or implied by context of the interaction to make it useful aka did not satisfy user's request and desire\n",
    "```\n",
    "\n",
    "\n",
    "# REFOCUS:\n",
    "- You are a code reviewer, not a language and contextual information content reviewer Do not mention issues not related to your purpose.\n",
    "- If the code was **unnecessary** aka user request FULLY satisfied without it, it can be absent and thus must receive null.\n",
    "- If code from assistant is necessary by the conversation flow to satisfy user's request but it is not there - score it as 1, do not mark as 5.\n",
    "- As you are giving a rating to a reply from a perfect AI Assistant, each issue decreases the rating/score significantly. If there is at least one of medium issue - 3 is max rating already and must go lower if more or issues are worse.\"\"\"\n",
    "\n",
    "\n",
    "chat_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"{main_prompt}\"\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\")\n",
    "    ])\n",
    "\n",
    "chat_template = chat_template.partial(main_prompt=main_prompt)\n",
    "\n",
    "with open('/Users/daniel/Desktop/Projects/TURING/Turing_Evaluator/Data/new_chirag/algorithm_problems_using_python__9__19_03_2024_05_12_22_1.ipynb', 'r') as file:\n",
    "    convo= file.read()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'scratchpad': 'The code provided by the assistant is mostly correct and performs the task as intended. However, there are a couple of issues that need attention.\\n\\nIssues identified:\\n1. Incorrect use of `np.all` with a generator expression.\\n2. Potential division by zero if `scores_std` is zero.\\n\\nBoth issues affect the correctness of the code.\\n\\nFinal Score: 3. The code is correct but can be significantly improved.', 'issues': [{'cell_position': 5, 'what': 'Incorrect use of `np.all` with a generator expression.', 'why': 'The `np.all` function does not work as intended with a generator expression. Instead, it should be used with a list comprehension.', 'where': 'Line: `if not np.all(isinstance(score, (int, float)) for score in scores):`', 'severity': 'Medium', 'fix': 'Change `np.all(isinstance(score, (int, float)) for score in scores)` to `np.all([isinstance(score, (int, float)) for score in scores])`.'}, {'cell_position': 5, 'what': 'Potential division by zero.', 'why': 'If `scores_std` is zero, there will be a division by zero error when calculating `scale_scores`.', 'where': 'Line: `scale_scores = target_std / scores_std if target_std else 1`', 'severity': 'Medium', 'fix': 'Add a check to handle the case where `scores_std` is zero. For example, `scale_scores = target_std / scores_std if target_std and scores_std != 0 else 1`.'}], 'scoring_explanation': 'The assistant provided code that is mostly correct but has significant issues that affect its correctness and efficiency. The code will not work as intended if the input scores contain non-numeric values or if the standard deviation of the scores is zero. These issues must be addressed to ensure the function works correctly in all scenarios.', 'score': 3}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = \"gpt-4o\"\n",
    "provider = \"openai_api\"\n",
    "\n",
    "\n",
    "evaluator = LLMModel2(\n",
    "provider=provider,\n",
    "model=model,\n",
    "output_schema=NotebookWiseFeedback.model_json_schema(),\n",
    "name=\"aspect_evaluator\",\n",
    "prompt_template=chat_template,\n",
    "try_to_parse=True,\n",
    "config={\n",
    "    \"retry\": 4,\n",
    "    \"params\":{\"temperature\":0.8}\n",
    "}\n",
    ")\n",
    "\n",
    "# Perform evaluation\n",
    "evaluation_result = evaluator( [HumanMessage((f\"Conversation between AI Assistant and a human User: \\n {convo}\"))])\n",
    "print(evaluation_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"line-height: 1.35;\"><span style=\"color: black;\">{</span>\n",
       "<span style=\"color: blue;\">    \"scratchpad\"</span>:<span style=\"color: green;\"> \"The code provided by the assistant is mostly correct and performs the task as intended. However, there are a couple of issues that need attention.\\n\\nIssues identified:\\n1. Incorrect use of `np.all` with a generator expression.\\n2. Potential division by zero if `scores_std` is zero.\\n\\nBoth issues affect the correctness of the code.\\n\\nFinal Score: 3. The code is correct but can be significantly improved.\",</span>\n",
       "<span style=\"color: blue;\">    \"issues\"</span>: [\n",
       "<span style=\"color: black;\">        {</span>\n",
       "<span style=\"color: blue;\">            \"cell_position\"</span>: 5,\n",
       "<span style=\"color: blue;\">            \"what\"</span>:<span style=\"color: green;\"> \"Incorrect use of `np.all` with a generator expression.\",</span>\n",
       "<span style=\"color: blue;\">            \"why\"</span>:<span style=\"color: green;\"> \"The `np.all` function does not work as intended with a generator expression. Instead, it should be used with a list comprehension.\",</span>\n",
       "<span style=\"color: blue;\">            \"where\"</span>:<span style=\"color: green;\"> \"Line: `if not np.all(isinstance(score, (int, float)) for score in scores):`\",</span>\n",
       "<span style=\"color: blue;\">            \"severity\"</span>:<span style=\"color: green;\"> \"Medium\",</span>\n",
       "<span style=\"color: blue;\">            \"fix\"</span>:<span style=\"color: green;\"> \"Change `np.all(isinstance(score, (int, float)) for score in scores)` to `np.all([isinstance(score, (int, float)) for score in scores])`.\"</span>\n",
       "<span style=\"color: black;\">        },</span>\n",
       "<span style=\"color: black;\">        {</span>\n",
       "<span style=\"color: blue;\">            \"cell_position\"</span>: 5,\n",
       "<span style=\"color: blue;\">            \"what\"</span>:<span style=\"color: green;\"> \"Potential division by zero.\",</span>\n",
       "<span style=\"color: blue;\">            \"why\"</span>:<span style=\"color: green;\"> \"If `scores_std` is zero, there will be a division by zero error when calculating `scale_scores`.\",</span>\n",
       "<span style=\"color: blue;\">            \"where\"</span>:<span style=\"color: green;\"> \"Line: `scale_scores = target_std / scores_std if target_std else 1`\",</span>\n",
       "<span style=\"color: blue;\">            \"severity\"</span>:<span style=\"color: green;\"> \"Medium\",</span>\n",
       "<span style=\"color: blue;\">            \"fix\"</span>:<span style=\"color: green;\"> \"Add a check to handle the case where `scores_std` is zero. For example, `scale_scores = target_std / scores_std if target_std and scores_std != 0 else 1`.\"</span>\n",
       "<span style=\"color: black;\">        }</span>\n",
       "<span style=\"color: black;\">    ],</span>\n",
       "<span style=\"color: blue;\">    \"scoring_explanation\"</span>:<span style=\"color: green;\"> \"The assistant provided code that is mostly correct but has significant issues that affect its correctness and efficiency. The code will not work as intended if the input scores contain non-numeric values or if the standard deviation of the scores is zero. These issues must be addressed to ensure the function works correctly in all scenarios.\",</span>\n",
       "<span style=\"color: blue;\">    \"score\"</span>:<span style=\"color: red;\"> 3</span>\n",
       "<span style=\"color: black;\">}</span>\n",
       "</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "import json\n",
    "\n",
    "def pretty_print_html(data):\n",
    "    # Convert the dictionary to a JSON string with indentation for readability\n",
    "    pretty_data = json.dumps(data, indent=4)\n",
    "    \n",
    "    # Prepare HTML string with color styling\n",
    "    html_data = '<pre style=\"line-height: 1.35;\">'\n",
    "    for line in pretty_data.splitlines():\n",
    "        if ':' in line:\n",
    "            key, value = line.split(':', 1)\n",
    "            key = f'<span style=\"color: blue;\">{key}</span>'\n",
    "            if value.strip().startswith('\"'):\n",
    "                value = f'<span style=\"color: green;\">{value}</span>'\n",
    "            elif value.strip().isdigit():\n",
    "                value = f'<span style=\"color: red;\">{value}</span>'\n",
    "            html_data += f\"{key}:{value}\\n\"\n",
    "        else:\n",
    "            html_data += f'<span style=\"color: black;\">{line}</span>\\n'\n",
    "    html_data += '</pre>'\n",
    "    \n",
    "    display(HTML(html_data))\n",
    "\n",
    "\n",
    "\n",
    "pretty_print_html(evaluation_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "evaluator",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
