[
  {
    "name": "Completeness",
    "description": "Overall conversation completeness",
    "prompt": "How complete is the conversation? Completeness is defined as:\n- The assistant always responds to the user.\n- The conversation contains at least 1 back and forth between the user and the assistant.\n- The conversation flow is not broken.",
    "role": "both"
  },
  {
    "name": "Project Scope Validity",
    "description": "Conversation alignment with the Project / Action",
    "prompt": "How much does the conversation align with the \"Project / Action\" value in metadata?\n- Write code in python: this should have the user make requests that elicit python code writing behavior from the assistant.\n- Explain code: this should have the user present medium/high complexity code to the assistant and have the assistant explain it\n- Fix / refactor / optimize code: this should have the user present medium/high complexity code to the assistant and have the assistant do modifications on it as requested.\n- Debug error trace: the user should present a stack trace and some code and the assistant will find what the problem is and potentially fix the code (It's okay to have situations where the bug is not in the presented code but in a dependency... though this should be rare).... This EXCLUDES having the assistant teach the user how to use debug tools to find what the problem is themselves\n- Write unit tests: this should have the user present some low/medium/high complexity code to the assistant and have the assistant write tests for it... maximizing test coverage. (Critical Path first, Corner Cases Second)\n- Write CI/CD code: this should have the user request some help from the assistant in writing ci/cd pipelines in any flavor. (Github actions, Gitlab, Jenkins... etc)\n- Do a code review: this should have the user present some code snippet and request the assistant to review the code as if it's a PR... providing high level conceptual feedback, modifying any bugs and using inline comments to mark changes or suggest alternatives.\n- Write / modify / fix beam code: this should have the user present some data schema or dummy data and have the assistant write beam code for it.\n- Write / modify / fix spark code: this should have the user present some data schema or dummy data and have the assistant write spark code for it.\n- Write end to end ML training code: scenarios where the conversation has the user and assistant solving a problem e2e data eda/prep, feature extraction, training, maybe some evals and visuals as well\n- Help me take an interview: scenario where the user requests the assistant to act as an interviewer and do a mock interview with a focus on a certain area... this should also include some final section where the assistant gives feedback to the user on how to be better... etc (Take inspiration from real interview questions, they should be at least medium complexity and occasionally challenging)\n- Answer ML research questions: this is where the user will ask some cutting edge conceptual questions related to ML Research Hot topics to the assistant... assistant can but is not obligated to provide code as a response.\n- Answer infra questions: user asks some conceptual or code snippet related questions within the scope of cloud, backend, database, development tools... all flavors are welcome!\n- Write / modify / fix SQL code: this should have the user elicit interaction from the assistant within the context of SQL code.\n- Write / modify / fix JavaScript code: this should have the user elicit interaction from the assistant within the context of Javascript code.\n- Scrape a website: this should have the user present some html and the assistant write code to scrape it.",
    "role": "both"
  },
  {
    "name": "Natural & Realistic",
    "description": "Resemble a real conversation and interactions a real user would have with a highly intelligent coding assistant ",
    "prompt": "How does the user interaction resemble a real conversation and interactions a real user would have with a highly intelligent coding assistant over chat.",
    "role": "human"
  },
  {
    "name": "Accuracy",
    "description": "Assistant code quality",
    "prompt": "How good is the code that the assistant generates.\nCode Qualities:\n#   - Correctness\n#   - Optimality\n#   - PEP8 Compliance & Readability\n\nHow good is the text that the assistant generates.\nText Qualities:\n#   - Spelling\n#   - Grammar\n#   - Capitalization & Punctuation",
    "role": "llm"
  },
  {
    "name": "Consumability",
    "description": "Assistant markdown quality",
    "prompt": "- How good is the markdown formatting that the assistant generates. Is it leveraging markdown syntax tools to maximize the readability of the text?\n- Information Density (Should be a sweet spot leaning on the concise side, but not too concise... definitely not too verbose)\n- Explains Code Well by adding comments tailored for the user level assuming a beginner user by default",
    "role": "llm"
  },
  {
    "name": "Engageness",
    "description": "Assistant conversation engagement",
    "prompt": "- How engaging is the assistant's messages? Does it keep the user engaged and interested in the conversation?\n- Does the assistant ask questions to the user to keep the conversation going?",
    "role": "llm"
  },
  {
    "name": "Right level of detail",
    "description": "Assistant level of detail",
    "prompt": "- How concise is the assistant's messages? Does it keep the messages short and to the point?\n- In case there are too many points to cover, does the assistant prioritize, emphasize the most helpful ones and provide a summary at the end for the rest?\n- Does the assistant avoid providing unsolicited information/code that is not helpful to the user?",
    "role": "llm"
  }
]
